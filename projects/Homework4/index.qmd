---
title: "Homework 4: Machine Learning"
author: "Alex Wissman"
date: today
jupyter: python3
---

:::: {.callout-note collapse="true"}
```{python}
import pandas as pd
import scipy as sp
import numpy as np
import statsmodels.api as sm
import pyrsm as rsm
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from sklearn.cluster import KMeans
```
::::


:::: {.callout-note collapse="true"}
```{python}
# Load and read the CSV file
penguins_df = pd.read_csv('/home/jovyan/mysite/palmer_penguins.csv')

# Display the first few rows of the dataframe
print(penguins_df.head())
```
::::

:::: {.callout-note collapse="true"}
```{python}

```
::::

_todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._


## 1a. K-Means

_todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working.  Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables.  Compare your results to the built-in `kmeans` function in R or Python._

```{python}
def kmeans_f(data, k, max_iters=100):
    np.random.seed(42)
    # Randomly initialize centroids
    centroids = data[np.random.choice(data.shape[0], k, replace=False)]
    for i in range(max_iters):
        # Assign clusters based on closest centroid
        distances = np.linalg.norm(data[:, None] - centroids[None, :], axis=2)
        labels = np.argmin(distances, axis=1)
        
        # Update centroids
        new_centroids = np.array([data[labels == cluster].mean(axis=0) for cluster in range(k)])
        
        # Check for convergence
        if np.all(centroids == new_centroids):
            break
        centroids = new_centroids
    return labels, centroids

# Prepare the data for clustering
numerical_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
data = penguins_df[numerical_columns].values

# Run the custom k-means algorithm
k = 3  # Number of clusters
labels, centroids = kmeans_f(data, k)

# Display the results
print("Cluster labels:", labels)
print("Centroids:", centroids)
```

:::: {.callout-note collapse="true"}
```{python}
# Select the bill length and flipper length columns for clustering
selected_columns = ['bill_length_mm', 'flipper_length_mm']
data_selected = penguins_df[selected_columns].values

# Run the custom k-means algorithm
labels_selected, centroids_selected = kmeans_f(data_selected, k)

# Display the results
print("Cluster labels (selected columns):", labels_selected)
print("Centroids (selected columns):", centroids_selected)
```
::::

:::: {.callout-note collapse="true"}
```{python}

# Perform clustering using the built-in KMeans function
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(data)

# Compare cluster labels and centroids
print("Built-in KMeans cluster labels:", kmeans.labels_)
print("Built-in KMeans centroids:", kmeans.cluster_centers_)

# Compare with custom implementation
print("Custom KMeans cluster labels:", labels)
print("Custom KMeans centroids:", centroids)

```
::::

Comparison of results:

Both the custom `kmeans_f` function and the built-in `sklearn.cluster.KMeans` function produce identical centroids and cluster labels for the full dataset (`data`). This indicates that the custom implementation is correctly replicating the behavior of the built-in function for clustering.

However, when clustering is performed on the selected columns (`data_selected`), the cluster labels and centroids differ between the two implementations. This discrepancy may arise due to differences in initialization, convergence criteria, or handling of the reduced dimensionality in the custom implementation compared to the built-in function.

_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). What is the "right" number of clusters as suggested by these two metrics?_



:::: {.callout-note collapse="true"}
```{python}
from sklearn.metrics import silhouette_score

# Initialize lists to store WCSS and silhouette scores
wcss = []
silhouette_scores = []

# Range of cluster numbers to evaluate
cluster_range = range(2, 8)

for k in cluster_range:
    # Perform clustering using KMeans
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data)
    
    # Calculate WCSS
    wcss.append(kmeans.inertia_)
    
    # Calculate silhouette score
    score = silhouette_score(data, kmeans.labels_)
    silhouette_scores.append(score)

# Plot WCSS and silhouette scores
plt.figure(figsize=(12, 6))

# Plot WCSS
plt.subplot(1, 2, 1)
plt.plot(cluster_range, wcss, marker='o')
plt.title('WCSS vs Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')

# Plot silhouette scores
plt.subplot(1, 2, 2)
plt.plot(cluster_range, silhouette_scores, marker='o')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')

plt.tight_layout()
plt.show()

#The "right" number of clusters can be determined by:
#1. The "elbow" point in the WCSS plot, where the reduction in WCSS slows down significantly.
#2. The peak of the silhouette score plot, indicating the highest average silhouette score.
```
::::

```{python}
# Plot WCSS and silhouette scores
plt.figure(figsize=(7, 2))

# Plot WCSS
plt.subplot(1, 2, 1)
plt.plot(cluster_range, wcss, marker='o')
plt.title('WCSS vs Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')

# Plot silhouette scores
plt.subplot(1, 2, 2)
plt.plot(cluster_range, silhouette_scores, marker='o')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')

plt.tight_layout()
plt.show()
```

_If you want a challenge, add your plots as an animated gif on your website so that the result looks something like [this](https://www.youtube.com/shorts/XCsoWZU9oN8)._



## 1b. Latent-Class MNL

_todo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57._

_The data provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current "wide" format into a "long" format._

_todo: Fit the standard MNL model on these data.  Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes._

_todo: How many classes are suggested by the $BIC = -2*\ell_n  + k*log(n)$? (where $\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) The Bayesian-Schwarz Information Criterion [link](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate -- akin to the adjusted R-squared for the linear regression model. Note, that a **lower** BIC indicates a better model fit, accounting for the number of parameters in the model._

_todo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC._



## 2a. K Nearest Neighbors

_todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function._

```{r}
# gen data -----
set.seed(42)
n <- 100
x1 <- runif(n, -3, 3)
x2 <- runif(n, -3, 3)
x <- cbind(x1, x2)

# define a wiggly boundary
boundary <- sin(4*x1) + x1
y <- ifelse(x2 > boundary, 1, 0) |> as.factor()
dat <- data.frame(x1 = x1, x2 = x2, y = y)
```

:::: {.callout-note collapse="true"}
```{python}
```
::::

_todo: plot the data where the horizontal axis is `x1`, the vertical axis is `x2`, and the points are colored by the value of `y`.  You may optionally draw the wiggly boundary._

:::: {.callout-note collapse="true"}
```{python}
```
::::

_todo: generate a test dataset with 100 points, using the same code as above but with a different seed._

:::: {.callout-note collapse="true"}
```{python}
```
::::

_todo: implement KNN by hand.  Check you work with a built-in function -- eg, `class::knn()` or `caret::train(method="knn")` in R, or scikit-learn's `KNeighborsClassifier` in Python._

:::: {.callout-note collapse="true"}
```{python}
```
::::

_todo: run your function for k=1,...,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?_ 

:::: {.callout-note collapse="true"}
```{python}
```
::::


## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._

