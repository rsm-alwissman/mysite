[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Alex’s Resume",
    "section": "",
    "text": "About this site\nDownload PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Wissman",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/HW1/index.html",
    "href": "projects/HW1/index.html",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nBy comparing the response rates and donation amounts across the three groups, Karlan and List were able to isolate the causal effect of different fundraising strategies on donor behavior. Their findings showed that matching grants significantly increased both the likelihood of giving and the average donation amount.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Homework 1: A Replication of Karlan and List (2007)\n\n\n\n\nAlex Wissman\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2: Poisson Regression Examples\n\n\n\n\nAlex Wissman\nMay 9, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. For at least one variable, perform the test as both t-test (use the formula in the class slides) and separately as a linear regression (regress for example mrm2 on treatment); confirm both methods yield the exact same results. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made (you can do this as a bivariate linear regression if you want). It may help to confirm your calculations match Table 2a Panel A. Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or something that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. This average will likely be “noisey” when only averaging a few numbers, but should “settle down” and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms at sample sizes 50, 200, 500, and 1000. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader."
  },
  {
    "objectID": "projects/HW1/index.html#introduction",
    "href": "projects/HW1/index.html#introduction",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nBy comparing the response rates and donation amounts across the three groups, Karlan and List were able to isolate the causal effect of different fundraising strategies on donor behavior. Their findings showed that matching grants significantly increased both the likelihood of giving and the average donation amount.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/HW1/index.html#data",
    "href": "projects/HW1/index.html#data",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\n\n# Load the data\ndataf = pd.read_stata(\"/home/jovyan/mysite/karlan_list_2007.dta\")\n\n# Display the first few rows of the dataset\nprint(dataf.head())\n\n   treatment  control    ratio  ...    powner  psch_atlstba pop_propurban\n0          0        1  Control  ...  0.499807      0.324528           1.0\n1          0        1  Control  ...       NaN           NaN           NaN\n2          1        0        1  ...  0.721941      0.192668           1.0\n3          1        0        1  ...  0.920431      0.412142           1.0\n4          1        0        1  ...  0.416072      0.439965           1.0\n\n[5 rows x 51 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# The following is a t-test of months since last donation to evaluate the difference between the treatment and control groups.\nfrom scipy.stats import ttest_ind\n\n# Filter the data for treatment and control groups\ntreatment_group = dataf[dataf['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = dataf[dataf['control'] == 1]['mrm2'].dropna()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(treatment_group, control_group, equal_var=False)\n\n# Check if the difference is statistically significant\nif p_value &lt; 0.05:\n    print(f\"The groups are significantly different (t={t_stat:.2f}, p={p_value:.4f}).\")\nelse:\n    print(f\"The groups are not significantly different (t={t_stat:.2f}, p={p_value:.4f}).\")\n\nThe groups are not significantly different (t=0.12, p=0.9049).\n\n\n\nimport pyrsm as rsm\n\nreg1 = rsm.model.regress({\"dataf\": dataf}, rvar=\"mrm2\", evar=[\"treatment\"])\nreg1.summary()\n\nLinear regression (OLS)\nData                 : dataf\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nNote: the same p-value is observed between the two statistical tests. Both results yield a p-value that indicates there is not a statistically significant difference between treatment and control at the 95% confidence level."
  },
  {
    "objectID": "projects/HW1/index.html#experimental-results",
    "href": "projects/HW1/index.html#experimental-results",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n# Calculate the proportion of people who donated in each group\ntreatment_donated_proportion = dataf[dataf['treatment'] == 1]['gave'].mean()\ncontrol_donated_proportion = dataf[dataf['control'] == 1]['gave'].mean()\n\n# Create a barplot\nbars = plt.bar(['Treatment', 'Control'], [treatment_donated_proportion, control_donated_proportion], color=['blue', 'orange'])\n\n# Add labels to show the values for each bar\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')\n\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of People Who Donated by Group')\nplt.show()\n\n\n\n\n\n\n\n\nIn the following code, I perform a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made.\n\ntreatment_gave = dataf[dataf['treatment'] == 1]['gave']\ncontrol_gave = dataf[dataf['control'] == 1]['gave']\n\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\n\nif p_value_gave &lt; 0.05:\n    print(f\"The groups are significantly different (t={t_stat_gave:.2f}, p={p_value_gave:.4f}).\")\nelse:\n    print(f\"The groups are not significantly different (t={t_stat_gave:.2f}, p={p_value_gave:.4f}).\")\n\nThe groups are significantly different (t=3.21, p=0.0013).\n\n\nWe found a statistically significant difference between the two groups in our experiment. The difference was strong enough that it’s very unlikely to have occurred by random chance alone. Based on these results., we find that the treatment had a meaningful impact on donation rates.\nThe following is a probit regression on donation outcome based on treatment/control assignment.\n\nimport statsmodels.api as sm\n\n# Define the outcome variable (gave) and the explanatory variable (treatment)\ndataf['intercept'] = 1  # Add an intercept for the regression\nexplanatory_vars = ['treatment', 'intercept']\noutcome_var = 'gave'\n\n# Fit the probit model\nprobit_model = sm.Probit(dataf[outcome_var], dataf[explanatory_vars])\nprobit_results = probit_model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n# Print the summary of the regression results\nprint(probit_results.summary())\n\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Fri, 09 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        22:25:46   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter the data for treatment group\ntreatment_data = dataf[dataf['treatment'] == 1]\n\n# Define the match ratio columns to test\nmatch_ratios = ['ratio', 'ratio2', 'ratio3']\n\n# Perform t-tests for each match ratio\nfor ratio in match_ratios:\n    # Ensure the column is numeric\n    treatment_data[ratio] = pd.to_numeric(treatment_data[ratio], errors='coerce')\n    \n    # Separate the groups based on whether 'gave' is 1 or 0\n    gave_1 = treatment_data[treatment_data['gave'] == 1][ratio].dropna()\n    gave_0 = treatment_data[treatment_data['gave'] == 0][ratio].dropna()\n    \n    # Perform the t-test\n    t_stat, p_value = ttest_ind(gave_1, gave_0, equal_var=False)\n    \n    # Print the results\n    print(f\"T-test for {ratio}: t-statistic = {t_stat:.2f}, p-value = {p_value:.4f}\")\n    if p_value &lt; 0.05:\n        print(f\"  The match ratio '{ratio}' has a significant effect on donation behavior.\")\n    else:\n        print(f\"  The match ratio '{ratio}' does not have a significant effect on donation behavior.\")\n\n&lt;string&gt;:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nT-test for ratio: t-statistic = 1.02, p-value = 0.3104\n  The match ratio 'ratio' does not have a significant effect on donation behavior.\nT-test for ratio2: t-statistic = 0.52, p-value = 0.6033\n  The match ratio 'ratio2' does not have a significant effect on donation behavior.\nT-test for ratio3: t-statistic = 0.61, p-value = 0.5443\n  The match ratio 'ratio3' does not have a significant effect on donation behavior.\n\n\nThese results support the “figures suggest” comment the authors make. We do not find that match threshold had a meaningful influence on behavior.\n\nimport statsmodels.api as sm\n\n# Create the variable 'ratio1'\ndataf['ratio1'] = dataf['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n# Define the explanatory variables and the outcome variable\nexplanatory_vars = ['ratio1', 'ratio2', 'ratio3', 'intercept']\noutcome_var = 'gave'\n\n# Drop rows with missing or infinite values in the explanatory variables\ndataf_cleaned = dataf[explanatory_vars + [outcome_var]].replace([np.inf, -np.inf], np.nan).dropna()\n\n# Fit the regression model\nregression_model = sm.Logit(dataf_cleaned[outcome_var], dataf_cleaned[explanatory_vars])\nregression_results = regression_model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100430\n         Iterations 8\n\n# Print the summary of the regression results\nprint(regression_results.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                          Logit   Df Residuals:                    50079\nMethod:                           MLE   Df Model:                            3\nDate:                Fri, 09 May 2025   Pseudo R-squ.:                0.001108\nTime:                        22:25:47   Log-Likelihood:                -5029.8\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                   0.01091\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.1530      0.089      1.728      0.084      -0.021       0.327\nratio2         0.2418      0.086      2.797      0.005       0.072       0.411\nratio3         0.2463      0.086      2.852      0.004       0.077       0.416\nintercept     -4.0073      0.058    -68.556      0.000      -4.122      -3.893\n==============================================================================\n\n\nThese results support the same findings that ratio levels do not influence donation behavior.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\n# Directly from the data\nresponse_rate_1_1 = dataf[dataf['ratio'] == 1]['gave'].mean()\nresponse_rate_2_1 = dataf[dataf['ratio'] == 2]['gave'].mean()\nresponse_rate_3_1 = dataf[dataf['ratio'] == 3]['gave'].mean()\n\n# Calculate the differences in response rates\ndiff_1_1_2_1 = response_rate_2_1 - response_rate_1_1\ndiff_2_1_3_1 = response_rate_3_1 - response_rate_2_1\n\nprint(f\"Response rate difference (1:1 vs 2:1): {diff_1_1_2_1:.4f}\")\n\nResponse rate difference (1:1 vs 2:1): 0.0019\n\nprint(f\"Response rate difference (2:1 vs 3:1): {diff_2_1_3_1:.4f}\")\n\nResponse rate difference (2:1 vs 3:1): 0.0001\n\n# Using the fitted coefficients\ncoef_ratio2 = regression_results.params['ratio2']\ncoef_ratio3 = regression_results.params['ratio3']\n\n# Calculate the differences in coefficients\ndiff_coef_1_1_2_1 = coef_ratio2\ndiff_coef_2_1_3_1 = coef_ratio3 - coef_ratio2\n\nprint(f\"Coefficient difference (1:1 vs 2:1): {diff_coef_1_1_2_1:.4f}\")\n\nCoefficient difference (1:1 vs 2:1): 0.2418\n\nprint(f\"Coefficient difference (2:1 vs 3:1): {diff_coef_2_1_3_1:.4f}\")\n\nCoefficient difference (2:1 vs 3:1): 0.0045\n\n\nConclusion: Regarding the effectiveness of different sizes of matched donations, the findings suggest that there is a larger difference in coefficients between 1:1 matching and 2:1 matching compared to the difference between 2:1 matching and 3:1 matching. This indicates that increasing the match size from 1:1 to 2:1 has a more pronounced effect on donation behavior than increasing it from 2:1 to 3:1.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy.stats import ttest_ind\n\nimport statsmodels.api as sm\n\n# Filter the data for non-missing donation amounts\ndataf_filtered = dataf.dropna(subset=['amount'])\n\n# Separate the donation amounts by treatment status\ntreatment_amount = dataf_filtered[dataf_filtered['treatment'] == 1]['amount']\ncontrol_amount = dataf_filtered[dataf_filtered['control'] == 1]['amount']\n\n# Perform a t-test\nt_stat_amount, p_value_amount = ttest_ind(treatment_amount, control_amount, equal_var=False)\nprint(f\"T-test results: t-statistic = {t_stat_amount:.2f}, p-value = {p_value_amount:.4f}\")\n\nT-test results: t-statistic = 1.92, p-value = 0.0551\n\n# Prepare data for regression\ndataf_filtered['intercept'] = 1  # Add intercept\nX = dataf_filtered[['treatment', 'intercept']]\ny = dataf_filtered['amount']\n\n# Fit a bivariate linear regression model\nregression_model_amount = sm.OLS(y, X).fit()\n\n# Print the regression summary\nprint(regression_model_amount.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Fri, 09 May 2025   Prob (F-statistic):             0.0628\nTime:                        22:25:47   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe analysis indicates that the treatment did not produce a statistically significant impact on the amount donated.\n\ndataf_positive_amount = dataf[dataf['amount'] &gt; 0]\nreg1 = rsm.model.regress({\"dataf_positive_amount\": dataf_positive_amount}, rvar=\"amount\", evar=[\"treatment\"])\nreg1.summary()\n\nLinear regression (OLS)\nData                 : dataf_positive_amount\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nHistogram plots of the donation amounts only among people who donated:\n\nimport matplotlib.pyplot as plt\n\n# Filter donation amounts for people who donated in each group\ntreatment_donations = treatment_amount[treatment_amount &gt; 0]\ncontrol_donations = control_amount[control_amount &gt; 0]\n\n# Calculate the sample averages\ntreatment_avg = treatment_donations.mean()\ncontrol_avg = control_donations.mean()\n\n# Create the histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donations, bins=30, color='blue', alpha=0.7, edgecolor='black')\naxes[0].axvline(treatment_avg, color='red', linestyle='dashed', linewidth=2, label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group Donations')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donations, bins=30, color='green', alpha=0.7, edgecolor='black')\naxes[1].axvline(control_avg, color='red', linestyle='dashed', linewidth=2, label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group Donations')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/HW1/index.html#simulation-experiment",
    "href": "projects/HW1/index.html#simulation-experiment",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set the probabilities for control and treatment groups\np_control = 0.018\np_treatment = 0.022\n\n# Simulate 10,000 draws from the Bernoulli distributions\ncontrol_draws = np.random.binomial(1, p_control, 10000)\ntreatment_draws = np.random.binomial(1, p_treatment, 10000)\n\n# Calculate the vector of differences\ndifferences = treatment_draws - control_draws\n\n# Calculate the cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='blue')\nplt.axhline(0, color='red', linestyle='--', label='Zero Line')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average')\nplt.title('Cumulative Average of Differences Between Treatment and Control')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe chart illustrates the cumulative average of the differences between the treatment and control groups over 10,000 simulations. Initially, when only a few numbers are averaged, the cumulative average is “noisy” and fluctuates significantly due to the small sample size. However, as the sample size increases, the cumulative average begins to stabilize and converge toward the true treatment effect, which is approximately 0.004 (calculated as 0.022 - 0.018). This behavior demonstrates the law of large numbers, where larger sample sizes reduce variability and provide a more accurate estimate of the true effect. The red dashed line at zero serves as a reference point, highlighting the positive treatment effect as the cumulative average settles above this line.\n\n\nCentral Limit Theorem\n\n# Set the probabilities for control and treatment groups\np_control = 0.018\np_treatment = 0.022\n\n# Define sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\n# Generate histograms for each sample size\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, p_control, sample_size)\n        treatment_sample = np.random.binomial(1, p_treatment, sample_size)\n        avg_difference = treatment_sample.mean() - control_sample.mean()\n        avg_differences.append(avg_difference)\n    \n    # Plot the histogram\n    axes[i].hist(avg_differences, bins=30, color='blue', alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n\n(array([  1.,   0.,   0.,   3.,   0.,   0.,  18.,   0.,   0.,  78.,   0.,\n         0., 190.,   0.,   0., 320.,   0., 248.,   0.,   0.,  97.,   1.,\n         0.,   0.,  32.,   0.,  10.,   0.,   0.,   2.]), array([-0.1       , -0.09333333, -0.08666667, -0.08      , -0.07333333,\n       -0.06666667, -0.06      , -0.05333333, -0.04666667, -0.04      ,\n       -0.03333333, -0.02666667, -0.02      , -0.01333333, -0.00666667,\n        0.        ,  0.00666667,  0.01333333,  0.02      ,  0.02666667,\n        0.03333333,  0.04      ,  0.04666667,  0.05333333,  0.06      ,\n        0.06666667,  0.07333333,  0.08      ,  0.08666667,  0.09333333,\n        0.1       ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 50')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  1.,   3.,   0.,   6.,   0.,   9.,  26.,   0.,  41.,   0.,  90.,\n       131.,   0., 152.,   0., 137., 131.,   0., 120.,   0.,  68.,  46.,\n         0.,  24.,   0.,   7.,   5.,   0.,   2.,   1.]), array([-0.04 , -0.037, -0.034, -0.031, -0.028, -0.025, -0.022, -0.019,\n       -0.016, -0.013, -0.01 , -0.007, -0.004, -0.001,  0.002,  0.005,\n        0.008,  0.011,  0.014,  0.017,  0.02 ,  0.023,  0.026,  0.029,\n        0.032,  0.035,  0.038,  0.041,  0.044,  0.047,  0.05 ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 200')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([ 1.,  1.,  3.,  2.,  5.,  4., 15., 13., 31., 75., 87., 65., 89.,\n       68., 98., 98., 73., 72., 70., 61., 25., 19., 11.,  6.,  2.,  1.,\n        3.,  0.,  1.,  1.]), array([-0.028 , -0.0258, -0.0236, -0.0214, -0.0192, -0.017 , -0.0148,\n       -0.0126, -0.0104, -0.0082, -0.006 , -0.0038, -0.0016,  0.0006,\n        0.0028,  0.005 ,  0.0072,  0.0094,  0.0116,  0.0138,  0.016 ,\n        0.0182,  0.0204,  0.0226,  0.0248,  0.027 ,  0.0292,  0.0314,\n        0.0336,  0.0358,  0.038 ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 500')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  1.,   0.,   0.,   1.,   6.,   2.,  15.,  17.,  34.,  26.,  72.,\n        45., 117.,  43., 143.,  65., 102.,  50.,  84.,  47.,  57.,  20.,\n        26.,   8.,   8.,   4.,   4.,   1.,   1.,   1.]), array([-0.018 , -0.0165, -0.015 , -0.0135, -0.012 , -0.0105, -0.009 ,\n       -0.0075, -0.006 , -0.0045, -0.003 , -0.0015,  0.    ,  0.0015,\n        0.003 ,  0.0045,  0.006 ,  0.0075,  0.009 ,  0.0105,  0.012 ,\n        0.0135,  0.015 ,  0.0165,  0.018 ,  0.0195,  0.021 ,  0.0225,\n        0.024 ,  0.0255,  0.027 ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 1000')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms illustrate the Central Limit Theorem (CLT): as sample size increases (50, 200, 500, 1000), the sampling distribution of the sample mean becomes smoother and more symmetric, converging toward a normal distribution. Larger sample sizes reduce variability and provide more precise estimates of the true mean.\n\nx &lt;- 5\nprint(x)\n\n[1] 5"
  },
  {
    "objectID": "projects/HW2/index.html",
    "href": "projects/HW2/index.html",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nimport pandas as pd"
  },
  {
    "objectID": "projects/HW2/index.html#blueprinty-case-study",
    "href": "projects/HW2/index.html#blueprinty-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nimport pandas as pd"
  },
  {
    "objectID": "projects/HW2/index.html#airbnb-case-study",
    "href": "projects/HW2/index.html#airbnb-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "hw2_questions.html",
    "href": "hw2_questions.html",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#blueprinty-case-study",
    "href": "hw2_questions.html#blueprinty-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#airbnb-case-study",
    "href": "hw2_questions.html#airbnb-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  }
]