[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Alex’s Resume",
    "section": "",
    "text": "About this site\nDownload PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Wissman",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/HW1/index.html",
    "href": "projects/HW1/index.html",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nBy comparing the response rates and donation amounts across the three groups, Karlan and List were able to isolate the causal effect of different fundraising strategies on donor behavior. Their findings showed that matching grants significantly increased both the likelihood of giving and the average donation amount.\nThis project seeks to replicate their results.\n\nimport sys\nprint(sys.executable)\n\n/opt/conda/bin/python3"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Homework 3: Multinomial Logit Model\n\n\n\n\nAlex Wissman\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Machine Learning\n\n\n\n\nAlex Wissman\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: A Replication of Karlan and List (2007)\n\n\n\n\nAlex Wissman\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2: Poisson Regression Examples\n\n\n\n\nAlex Wissman\nJun 10, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. For at least one variable, perform the test as both t-test (use the formula in the class slides) and separately as a linear regression (regress for example mrm2 on treatment); confirm both methods yield the exact same results. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made (you can do this as a bivariate linear regression if you want). It may help to confirm your calculations match Table 2a Panel A. Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or something that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. This average will likely be “noisey” when only averaging a few numbers, but should “settle down” and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms at sample sizes 50, 200, 500, and 1000. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader."
  },
  {
    "objectID": "projects/HW1/index.html#introduction",
    "href": "projects/HW1/index.html#introduction",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nBy comparing the response rates and donation amounts across the three groups, Karlan and List were able to isolate the causal effect of different fundraising strategies on donor behavior. Their findings showed that matching grants significantly increased both the likelihood of giving and the average donation amount.\nThis project seeks to replicate their results.\n\nimport sys\nprint(sys.executable)\n\n/opt/conda/bin/python3"
  },
  {
    "objectID": "projects/HW1/index.html#data",
    "href": "projects/HW1/index.html#data",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\n\n# Load the data\ndataf = pd.read_stata(\"/home/jovyan/mysite/karlan_list_2007.dta\")\n\n# Display the first few rows of the dataset\nprint(dataf.head())\n\n   treatment  control    ratio  ...    powner  psch_atlstba pop_propurban\n0          0        1  Control  ...  0.499807      0.324528           1.0\n1          0        1  Control  ...       NaN           NaN           NaN\n2          1        0        1  ...  0.721941      0.192668           1.0\n3          1        0        1  ...  0.920431      0.412142           1.0\n4          1        0        1  ...  0.416072      0.439965           1.0\n\n[5 rows x 51 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# The following is a t-test of months since last donation to evaluate the difference between the treatment and control groups.\nfrom scipy.stats import ttest_ind\n\n# Filter the data for treatment and control groups\ntreatment_group = dataf[dataf['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = dataf[dataf['control'] == 1]['mrm2'].dropna()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(treatment_group, control_group, equal_var=False)\n\n# Check if the difference is statistically significant\nif p_value &lt; 0.05:\n    print(f\"The groups are significantly different (t={t_stat:.2f}, p={p_value:.4f}).\")\nelse:\n    print(f\"The groups are not significantly different (t={t_stat:.2f}, p={p_value:.4f}).\")\n\nThe groups are not significantly different (t=0.12, p=0.9049).\n\n\n\nimport pyrsm as rsm\n\nreg1 = rsm.model.regress({\"dataf\": dataf}, rvar=\"mrm2\", evar=[\"treatment\"])\nreg1.summary()\n\nLinear regression (OLS)\nData                 : dataf\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nNote: the same p-value is observed between the two statistical tests. Both results yield a p-value that indicates there is not a statistically significant difference between treatment and control at the 95% confidence level."
  },
  {
    "objectID": "projects/HW1/index.html#experimental-results",
    "href": "projects/HW1/index.html#experimental-results",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n# Calculate the proportion of people who donated in each group\ntreatment_donated_proportion = dataf[dataf['treatment'] == 1]['gave'].mean()\ncontrol_donated_proportion = dataf[dataf['control'] == 1]['gave'].mean()\n\n# Create a barplot\nbars = plt.bar(['Treatment', 'Control'], [treatment_donated_proportion, control_donated_proportion], color=['blue', 'orange'])\n\n# Add labels to show the values for each bar\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')\n\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of People Who Donated by Group')\nplt.show()\n\n\n\n\n\n\n\n\nIn the following code, I perform a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made.\n\ntreatment_gave = dataf[dataf['treatment'] == 1]['gave']\ncontrol_gave = dataf[dataf['control'] == 1]['gave']\n\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\n\nif p_value_gave &lt; 0.05:\n    print(f\"The groups are significantly different (t={t_stat_gave:.2f}, p={p_value_gave:.4f}).\")\nelse:\n    print(f\"The groups are not significantly different (t={t_stat_gave:.2f}, p={p_value_gave:.4f}).\")\n\nThe groups are significantly different (t=3.21, p=0.0013).\n\n\nWe found a statistically significant difference between the two groups in our experiment. The difference was strong enough that it’s very unlikely to have occurred by random chance alone. Based on these results., we find that the treatment had a meaningful impact on donation rates.\nThe following is a probit regression on donation outcome based on treatment/control assignment.\n\nimport statsmodels.api as sm\n\n# Define the outcome variable (gave) and the explanatory variable (treatment)\ndataf['intercept'] = 1  # Add an intercept for the regression\nexplanatory_vars = ['treatment', 'intercept']\noutcome_var = 'gave'\n\n# Fit the probit model\nprobit_model = sm.Probit(dataf[outcome_var], dataf[explanatory_vars])\nprobit_results = probit_model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n# Print the summary of the regression results\nprint(probit_results.summary())\n\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 12 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        14:23:00   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter the data for treatment group\ntreatment_data = dataf[dataf['treatment'] == 1]\n\n# Define the match ratio columns to test\nmatch_ratios = ['ratio', 'ratio2', 'ratio3']\n\n# Perform t-tests for each match ratio\nfor ratio in match_ratios:\n    # Ensure the column is numeric\n    treatment_data[ratio] = pd.to_numeric(treatment_data[ratio], errors='coerce')\n    \n    # Separate the groups based on whether 'gave' is 1 or 0\n    gave_1 = treatment_data[treatment_data['gave'] == 1][ratio].dropna()\n    gave_0 = treatment_data[treatment_data['gave'] == 0][ratio].dropna()\n    \n    # Perform the t-test\n    t_stat, p_value = ttest_ind(gave_1, gave_0, equal_var=False)\n    \n    # Print the results\n    print(f\"T-test for {ratio}: t-statistic = {t_stat:.2f}, p-value = {p_value:.4f}\")\n    if p_value &lt; 0.05:\n        print(f\"  The match ratio '{ratio}' has a significant effect on donation behavior.\")\n    else:\n        print(f\"  The match ratio '{ratio}' does not have a significant effect on donation behavior.\")\n\n&lt;string&gt;:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nT-test for ratio: t-statistic = 1.02, p-value = 0.3104\n  The match ratio 'ratio' does not have a significant effect on donation behavior.\nT-test for ratio2: t-statistic = 0.52, p-value = 0.6033\n  The match ratio 'ratio2' does not have a significant effect on donation behavior.\nT-test for ratio3: t-statistic = 0.61, p-value = 0.5443\n  The match ratio 'ratio3' does not have a significant effect on donation behavior.\n\n\nThese results support the “figures suggest” comment the authors make. We do not find that match threshold had a meaningful influence on behavior.\n\nimport statsmodels.api as sm\n\n# Create the variable 'ratio1'\ndataf['ratio1'] = dataf['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n# Define the explanatory variables and the outcome variable\nexplanatory_vars = ['ratio1', 'ratio2', 'ratio3', 'intercept']\noutcome_var = 'gave'\n\n# Drop rows with missing or infinite values in the explanatory variables\ndataf_cleaned = dataf[explanatory_vars + [outcome_var]].replace([np.inf, -np.inf], np.nan).dropna()\n\n# Fit the regression model\nregression_model = sm.Logit(dataf_cleaned[outcome_var], dataf_cleaned[explanatory_vars])\nregression_results = regression_model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100430\n         Iterations 8\n\n# Print the summary of the regression results\nprint(regression_results.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                          Logit   Df Residuals:                    50079\nMethod:                           MLE   Df Model:                            3\nDate:                Mon, 12 May 2025   Pseudo R-squ.:                0.001108\nTime:                        14:23:00   Log-Likelihood:                -5029.8\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                   0.01091\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.1530      0.089      1.728      0.084      -0.021       0.327\nratio2         0.2418      0.086      2.797      0.005       0.072       0.411\nratio3         0.2463      0.086      2.852      0.004       0.077       0.416\nintercept     -4.0073      0.058    -68.556      0.000      -4.122      -3.893\n==============================================================================\n\n\nThese results support the same findings that ratio levels do not influence donation behavior.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\n# Directly from the data\nresponse_rate_1_1 = dataf[dataf['ratio'] == 1]['gave'].mean()\nresponse_rate_2_1 = dataf[dataf['ratio'] == 2]['gave'].mean()\nresponse_rate_3_1 = dataf[dataf['ratio'] == 3]['gave'].mean()\n\n# Calculate the differences in response rates\ndiff_1_1_2_1 = response_rate_2_1 - response_rate_1_1\ndiff_2_1_3_1 = response_rate_3_1 - response_rate_2_1\n\nprint(f\"Response rate difference (1:1 vs 2:1): {diff_1_1_2_1:.4f}\")\n\nResponse rate difference (1:1 vs 2:1): 0.0019\n\nprint(f\"Response rate difference (2:1 vs 3:1): {diff_2_1_3_1:.4f}\")\n\nResponse rate difference (2:1 vs 3:1): 0.0001\n\n# Using the fitted coefficients\ncoef_ratio2 = regression_results.params['ratio2']\ncoef_ratio3 = regression_results.params['ratio3']\n\n# Calculate the differences in coefficients\ndiff_coef_1_1_2_1 = coef_ratio2\ndiff_coef_2_1_3_1 = coef_ratio3 - coef_ratio2\n\nprint(f\"Coefficient difference (1:1 vs 2:1): {diff_coef_1_1_2_1:.4f}\")\n\nCoefficient difference (1:1 vs 2:1): 0.2418\n\nprint(f\"Coefficient difference (2:1 vs 3:1): {diff_coef_2_1_3_1:.4f}\")\n\nCoefficient difference (2:1 vs 3:1): 0.0045\n\n\nConclusion: Regarding the effectiveness of different sizes of matched donations, the findings suggest that there is a larger difference in coefficients between 1:1 matching and 2:1 matching compared to the difference between 2:1 matching and 3:1 matching. This indicates that increasing the match size from 1:1 to 2:1 has a more pronounced effect on donation behavior than increasing it from 2:1 to 3:1.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy.stats import ttest_ind\n\nimport statsmodels.api as sm\n\n# Filter the data for non-missing donation amounts\ndataf_filtered = dataf.dropna(subset=['amount'])\n\n# Separate the donation amounts by treatment status\ntreatment_amount = dataf_filtered[dataf_filtered['treatment'] == 1]['amount']\ncontrol_amount = dataf_filtered[dataf_filtered['control'] == 1]['amount']\n\n# Perform a t-test\nt_stat_amount, p_value_amount = ttest_ind(treatment_amount, control_amount, equal_var=False)\nprint(f\"T-test results: t-statistic = {t_stat_amount:.2f}, p-value = {p_value_amount:.4f}\")\n\nT-test results: t-statistic = 1.92, p-value = 0.0551\n\n# Prepare data for regression\ndataf_filtered['intercept'] = 1  # Add intercept\nX = dataf_filtered[['treatment', 'intercept']]\ny = dataf_filtered['amount']\n\n# Fit a bivariate linear regression model\nregression_model_amount = sm.OLS(y, X).fit()\n\n# Print the regression summary\nprint(regression_model_amount.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Mon, 12 May 2025   Prob (F-statistic):             0.0628\nTime:                        14:23:00   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe analysis indicates that the treatment did not produce a statistically significant impact on the amount donated.\n\ndataf_positive_amount = dataf[dataf['amount'] &gt; 0]\nreg1 = rsm.model.regress({\"dataf_positive_amount\": dataf_positive_amount}, rvar=\"amount\", evar=[\"treatment\"])\nreg1.summary()\n\nLinear regression (OLS)\nData                 : dataf_positive_amount\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nHistogram plots of the donation amounts only among people who donated:\n\nimport matplotlib.pyplot as plt\n\n# Filter donation amounts for people who donated in each group\ntreatment_donations = treatment_amount[treatment_amount &gt; 0]\ncontrol_donations = control_amount[control_amount &gt; 0]\n\n# Calculate the sample averages\ntreatment_avg = treatment_donations.mean()\ncontrol_avg = control_donations.mean()\n\n# Create the histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donations, bins=30, color='blue', alpha=0.7, edgecolor='black')\naxes[0].axvline(treatment_avg, color='red', linestyle='dashed', linewidth=2, label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group Donations')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donations, bins=30, color='green', alpha=0.7, edgecolor='black')\naxes[1].axvline(control_avg, color='red', linestyle='dashed', linewidth=2, label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group Donations')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/HW1/index.html#simulation-experiment",
    "href": "projects/HW1/index.html#simulation-experiment",
    "title": "Homework 1: A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set the probabilities for control and treatment groups\np_control = 0.018\np_treatment = 0.022\n\n# Simulate 10,000 draws from the Bernoulli distributions\ncontrol_draws = np.random.binomial(1, p_control, 10000)\ntreatment_draws = np.random.binomial(1, p_treatment, 10000)\n\n# Calculate the vector of differences\ndifferences = treatment_draws - control_draws\n\n# Calculate the cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='blue')\nplt.axhline(0, color='red', linestyle='--', label='Zero Line')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average')\nplt.title('Cumulative Average of Differences Between Treatment and Control')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe chart illustrates the cumulative average of the differences between the treatment and control groups over 10,000 simulations. Initially, when only a few numbers are averaged, the cumulative average is “noisy” and fluctuates significantly due to the small sample size. However, as the sample size increases, the cumulative average begins to stabilize and converge toward the true treatment effect, which is approximately 0.004 (calculated as 0.022 - 0.018). This behavior demonstrates the law of large numbers, where larger sample sizes reduce variability and provide a more accurate estimate of the true effect. The red dashed line at zero serves as a reference point, highlighting the positive treatment effect as the cumulative average settles above this line.\n\n\nCentral Limit Theorem\n\n# Set the probabilities for control and treatment groups\np_control = 0.018\np_treatment = 0.022\n\n# Define sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\n# Generate histograms for each sample size\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, p_control, sample_size)\n        treatment_sample = np.random.binomial(1, p_treatment, sample_size)\n        avg_difference = treatment_sample.mean() - control_sample.mean()\n        avg_differences.append(avg_difference)\n    \n    # Plot the histogram\n    axes[i].hist(avg_differences, bins=30, color='blue', alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n\n(array([  3.,   0.,   0.,  23.,   0.,   0.,   0.,  70.,   0.,   0.,   0.,\n       174.,   0.,   0.,   0., 301.,   0.,   0., 256.,   0.,   0.,   0.,\n       125.,   0.,   0.,   0.,  43.,   0.,   0.,   5.]), array([-0.08      , -0.07466667, -0.06933333, -0.064     , -0.05866667,\n       -0.05333333, -0.048     , -0.04266667, -0.03733333, -0.032     ,\n       -0.02666667, -0.02133333, -0.016     , -0.01066667, -0.00533333,\n        0.        ,  0.00533333,  0.01066667,  0.016     ,  0.02133333,\n        0.02666667,  0.032     ,  0.03733333,  0.04266667,  0.048     ,\n        0.05333333,  0.05866667,  0.064     ,  0.06933333,  0.07466667,\n        0.08      ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 50')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  1.,   0.,   0.,   0.,   2.,   0.,   6.,   3.,   8.,   5.,  31.,\n        27.,  28.,  86.,  89.,  14., 150., 132.,   5., 131., 110.,   0.,\n        74.,  26.,  27.,  21.,  17.,   0.,   5.,   2.]), array([-0.055     , -0.05166667, -0.04833333, -0.045     , -0.04166667,\n       -0.03833333, -0.035     , -0.03166667, -0.02833333, -0.025     ,\n       -0.02166667, -0.01833333, -0.015     , -0.01166667, -0.00833333,\n       -0.005     , -0.00166667,  0.00166667,  0.005     ,  0.00833333,\n        0.01166667,  0.015     ,  0.01833333,  0.02166667,  0.025     ,\n        0.02833333,  0.03166667,  0.035     ,  0.03833333,  0.04166667,\n        0.045     ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 200')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  1.,   1.,   0.,   2.,   5.,   5.,  22.,  16.,  36.,  35.,  26.,\n        85.,  76.,  77.,  49., 118.,  97.,  79.,  55.,  59.,  55.,   2.,\n        40.,  25.,  16.,   8.,   3.,   4.,   2.,   1.]), array([-2.80000000e-02, -2.60000000e-02, -2.40000000e-02, -2.20000000e-02,\n       -2.00000000e-02, -1.80000000e-02, -1.60000000e-02, -1.40000000e-02,\n       -1.20000000e-02, -1.00000000e-02, -8.00000000e-03, -6.00000000e-03,\n       -4.00000000e-03, -2.00000000e-03,  3.46944695e-18,  2.00000000e-03,\n        4.00000000e-03,  6.00000000e-03,  8.00000000e-03,  1.00000000e-02,\n        1.20000000e-02,  1.40000000e-02,  1.60000000e-02,  1.80000000e-02,\n        2.00000000e-02,  2.20000000e-02,  2.40000000e-02,  2.60000000e-02,\n        2.80000000e-02,  3.00000000e-02,  3.20000000e-02]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 500')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  3.,   3.,   5.,   4.,   6.,  32.,  29.,  32.,  33.,  43., 102.,\n        44.,  66.,  69.,  75.,  92.,  55.,  58.,  40.,  48.,  64.,  26.,\n        13.,  19.,  13.,  14.,   6.,   2.,   1.,   3.]), array([-0.013 , -0.0118, -0.0106, -0.0094, -0.0082, -0.007 , -0.0058,\n       -0.0046, -0.0034, -0.0022, -0.001 ,  0.0002,  0.0014,  0.0026,\n        0.0038,  0.005 ,  0.0062,  0.0074,  0.0086,  0.0098,  0.011 ,\n        0.0122,  0.0134,  0.0146,  0.0158,  0.017 ,  0.0182,  0.0194,\n        0.0206,  0.0218,  0.023 ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 1000')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms illustrate the Central Limit Theorem (CLT): as sample size increases (50, 200, 500, 1000), the sampling distribution of the sample mean becomes smoother and more symmetric, converging toward a normal distribution. Larger sample sizes reduce variability and provide more precise estimates of the true mean.\n\nx &lt;- 5\nprint(x)\n\n[1] 5"
  },
  {
    "objectID": "projects/HW2/index.html",
    "href": "projects/HW2/index.html",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved."
  },
  {
    "objectID": "projects/HW2/index.html#blueprinty-case-study",
    "href": "projects/HW2/index.html#blueprinty-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved."
  },
  {
    "objectID": "projects/HW2/index.html#airbnb-case-study",
    "href": "projects/HW2/index.html#airbnb-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "hw2_questions.html",
    "href": "hw2_questions.html",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#blueprinty-case-study",
    "href": "hw2_questions.html#blueprinty-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#airbnb-case-study",
    "href": "hw2_questions.html#airbnb-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "projects/HW2/index copy.html",
    "href": "projects/HW2/index copy.html",
    "title": "Homework 2: A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nBy comparing the response rates and donation amounts across the three groups, Karlan and List were able to isolate the causal effect of different fundraising strategies on donor behavior. Their findings showed that matching grants significantly increased both the likelihood of giving and the average donation amount.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/HW2/index copy.html#introduction",
    "href": "projects/HW2/index copy.html#introduction",
    "title": "Homework 2: A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nBy comparing the response rates and donation amounts across the three groups, Karlan and List were able to isolate the causal effect of different fundraising strategies on donor behavior. Their findings showed that matching grants significantly increased both the likelihood of giving and the average donation amount.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/HW2/index copy.html#data",
    "href": "projects/HW2/index copy.html#data",
    "title": "Homework 2: A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\n\n# Load the data\ndataf = pd.read_stata(\"/home/jovyan/mysite/karlan_list_2007.dta\")\n\n# Display the first few rows of the dataset\nprint(dataf.head())\n\n   treatment  control    ratio  ...    powner  psch_atlstba pop_propurban\n0          0        1  Control  ...  0.499807      0.324528           1.0\n1          0        1  Control  ...       NaN           NaN           NaN\n2          1        0        1  ...  0.721941      0.192668           1.0\n3          1        0        1  ...  0.920431      0.412142           1.0\n4          1        0        1  ...  0.416072      0.439965           1.0\n\n[5 rows x 51 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# The following is a t-test of months since last donation to evaluate the difference between the treatment and control groups.\nfrom scipy.stats import ttest_ind\n\n# Filter the data for treatment and control groups\ntreatment_group = dataf[dataf['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = dataf[dataf['control'] == 1]['mrm2'].dropna()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(treatment_group, control_group, equal_var=False)\n\n# Check if the difference is statistically significant\nif p_value &lt; 0.05:\n    print(f\"The groups are significantly different (t={t_stat:.2f}, p={p_value:.4f}).\")\nelse:\n    print(f\"The groups are not significantly different (t={t_stat:.2f}, p={p_value:.4f}).\")\n\nThe groups are not significantly different (t=0.12, p=0.9049).\n\n\n\nimport pyrsm as rsm\n\nreg1 = rsm.model.regress({\"dataf\": dataf}, rvar=\"mrm2\", evar=[\"treatment\"])\nreg1.summary()\n\nLinear regression (OLS)\nData                 : dataf\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nNote: the same p-value is observed between the two statistical tests. Both results yield a p-value that indicates there is not a statistically significant difference between treatment and control at the 95% confidence level."
  },
  {
    "objectID": "projects/HW2/index copy.html#experimental-results",
    "href": "projects/HW2/index copy.html#experimental-results",
    "title": "Homework 2: A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n# Calculate the proportion of people who donated in each group\ntreatment_donated_proportion = dataf[dataf['treatment'] == 1]['gave'].mean()\ncontrol_donated_proportion = dataf[dataf['control'] == 1]['gave'].mean()\n\n# Create a barplot\nbars = plt.bar(['Treatment', 'Control'], [treatment_donated_proportion, control_donated_proportion], color=['blue', 'orange'])\n\n# Add labels to show the values for each bar\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')\n\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of People Who Donated by Group')\nplt.show()\n\n\n\n\n\n\n\n\nIn the following code, I perform a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made.\n\ntreatment_gave = dataf[dataf['treatment'] == 1]['gave']\ncontrol_gave = dataf[dataf['control'] == 1]['gave']\n\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\n\nif p_value_gave &lt; 0.05:\n    print(f\"The groups are significantly different (t={t_stat_gave:.2f}, p={p_value_gave:.4f}).\")\nelse:\n    print(f\"The groups are not significantly different (t={t_stat_gave:.2f}, p={p_value_gave:.4f}).\")\n\nThe groups are significantly different (t=3.21, p=0.0013).\n\n\nWe found a statistically significant difference between the two groups in our experiment. The difference was strong enough that it’s very unlikely to have occurred by random chance alone. Based on these results., we find that the treatment had a meaningful impact on donation rates.\nThe following is a probit regression on donation outcome based on treatment/control assignment.\n\nimport statsmodels.api as sm\n\n# Define the outcome variable (gave) and the explanatory variable (treatment)\ndataf['intercept'] = 1  # Add an intercept for the regression\nexplanatory_vars = ['treatment', 'intercept']\noutcome_var = 'gave'\n\n# Fit the probit model\nprobit_model = sm.Probit(dataf[outcome_var], dataf[explanatory_vars])\nprobit_results = probit_model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n# Print the summary of the regression results\nprint(probit_results.summary())\n\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 12 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        13:56:54   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter the data for treatment group\ntreatment_data = dataf[dataf['treatment'] == 1]\n\n# Define the match ratio columns to test\nmatch_ratios = ['ratio', 'ratio2', 'ratio3']\n\n# Perform t-tests for each match ratio\nfor ratio in match_ratios:\n    # Ensure the column is numeric\n    treatment_data[ratio] = pd.to_numeric(treatment_data[ratio], errors='coerce')\n    \n    # Separate the groups based on whether 'gave' is 1 or 0\n    gave_1 = treatment_data[treatment_data['gave'] == 1][ratio].dropna()\n    gave_0 = treatment_data[treatment_data['gave'] == 0][ratio].dropna()\n    \n    # Perform the t-test\n    t_stat, p_value = ttest_ind(gave_1, gave_0, equal_var=False)\n    \n    # Print the results\n    print(f\"T-test for {ratio}: t-statistic = {t_stat:.2f}, p-value = {p_value:.4f}\")\n    if p_value &lt; 0.05:\n        print(f\"  The match ratio '{ratio}' has a significant effect on donation behavior.\")\n    else:\n        print(f\"  The match ratio '{ratio}' does not have a significant effect on donation behavior.\")\n\n&lt;string&gt;:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nT-test for ratio: t-statistic = 1.02, p-value = 0.3104\n  The match ratio 'ratio' does not have a significant effect on donation behavior.\nT-test for ratio2: t-statistic = 0.52, p-value = 0.6033\n  The match ratio 'ratio2' does not have a significant effect on donation behavior.\nT-test for ratio3: t-statistic = 0.61, p-value = 0.5443\n  The match ratio 'ratio3' does not have a significant effect on donation behavior.\n\n\nThese results support the “figures suggest” comment the authors make. We do not find that match threshold had a meaningful influence on behavior.\n\nimport statsmodels.api as sm\n\n# Create the variable 'ratio1'\ndataf['ratio1'] = dataf['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n# Define the explanatory variables and the outcome variable\nexplanatory_vars = ['ratio1', 'ratio2', 'ratio3', 'intercept']\noutcome_var = 'gave'\n\n# Drop rows with missing or infinite values in the explanatory variables\ndataf_cleaned = dataf[explanatory_vars + [outcome_var]].replace([np.inf, -np.inf], np.nan).dropna()\n\n# Fit the regression model\nregression_model = sm.Logit(dataf_cleaned[outcome_var], dataf_cleaned[explanatory_vars])\nregression_results = regression_model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100430\n         Iterations 8\n\n# Print the summary of the regression results\nprint(regression_results.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                          Logit   Df Residuals:                    50079\nMethod:                           MLE   Df Model:                            3\nDate:                Mon, 12 May 2025   Pseudo R-squ.:                0.001108\nTime:                        13:56:55   Log-Likelihood:                -5029.8\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                   0.01091\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.1530      0.089      1.728      0.084      -0.021       0.327\nratio2         0.2418      0.086      2.797      0.005       0.072       0.411\nratio3         0.2463      0.086      2.852      0.004       0.077       0.416\nintercept     -4.0073      0.058    -68.556      0.000      -4.122      -3.893\n==============================================================================\n\n\nThese results support the same findings that ratio levels do not influence donation behavior.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\n# Directly from the data\nresponse_rate_1_1 = dataf[dataf['ratio'] == 1]['gave'].mean()\nresponse_rate_2_1 = dataf[dataf['ratio'] == 2]['gave'].mean()\nresponse_rate_3_1 = dataf[dataf['ratio'] == 3]['gave'].mean()\n\n# Calculate the differences in response rates\ndiff_1_1_2_1 = response_rate_2_1 - response_rate_1_1\ndiff_2_1_3_1 = response_rate_3_1 - response_rate_2_1\n\nprint(f\"Response rate difference (1:1 vs 2:1): {diff_1_1_2_1:.4f}\")\n\nResponse rate difference (1:1 vs 2:1): 0.0019\n\nprint(f\"Response rate difference (2:1 vs 3:1): {diff_2_1_3_1:.4f}\")\n\nResponse rate difference (2:1 vs 3:1): 0.0001\n\n# Using the fitted coefficients\ncoef_ratio2 = regression_results.params['ratio2']\ncoef_ratio3 = regression_results.params['ratio3']\n\n# Calculate the differences in coefficients\ndiff_coef_1_1_2_1 = coef_ratio2\ndiff_coef_2_1_3_1 = coef_ratio3 - coef_ratio2\n\nprint(f\"Coefficient difference (1:1 vs 2:1): {diff_coef_1_1_2_1:.4f}\")\n\nCoefficient difference (1:1 vs 2:1): 0.2418\n\nprint(f\"Coefficient difference (2:1 vs 3:1): {diff_coef_2_1_3_1:.4f}\")\n\nCoefficient difference (2:1 vs 3:1): 0.0045\n\n\nConclusion: Regarding the effectiveness of different sizes of matched donations, the findings suggest that there is a larger difference in coefficients between 1:1 matching and 2:1 matching compared to the difference between 2:1 matching and 3:1 matching. This indicates that increasing the match size from 1:1 to 2:1 has a more pronounced effect on donation behavior than increasing it from 2:1 to 3:1.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy.stats import ttest_ind\n\nimport statsmodels.api as sm\n\n# Filter the data for non-missing donation amounts\ndataf_filtered = dataf.dropna(subset=['amount'])\n\n# Separate the donation amounts by treatment status\ntreatment_amount = dataf_filtered[dataf_filtered['treatment'] == 1]['amount']\ncontrol_amount = dataf_filtered[dataf_filtered['control'] == 1]['amount']\n\n# Perform a t-test\nt_stat_amount, p_value_amount = ttest_ind(treatment_amount, control_amount, equal_var=False)\nprint(f\"T-test results: t-statistic = {t_stat_amount:.2f}, p-value = {p_value_amount:.4f}\")\n\nT-test results: t-statistic = 1.92, p-value = 0.0551\n\n# Prepare data for regression\ndataf_filtered['intercept'] = 1  # Add intercept\nX = dataf_filtered[['treatment', 'intercept']]\ny = dataf_filtered['amount']\n\n# Fit a bivariate linear regression model\nregression_model_amount = sm.OLS(y, X).fit()\n\n# Print the regression summary\nprint(regression_model_amount.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Mon, 12 May 2025   Prob (F-statistic):             0.0628\nTime:                        13:56:55   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe analysis indicates that the treatment did not produce a statistically significant impact on the amount donated.\n\ndataf_positive_amount = dataf[dataf['amount'] &gt; 0]\nreg1 = rsm.model.regress({\"dataf_positive_amount\": dataf_positive_amount}, rvar=\"amount\", evar=[\"treatment\"])\nreg1.summary()\n\nLinear regression (OLS)\nData                 : dataf_positive_amount\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nHistogram plots of the donation amounts only among people who donated:\n\nimport matplotlib.pyplot as plt\n\n# Filter donation amounts for people who donated in each group\ntreatment_donations = treatment_amount[treatment_amount &gt; 0]\ncontrol_donations = control_amount[control_amount &gt; 0]\n\n# Calculate the sample averages\ntreatment_avg = treatment_donations.mean()\ncontrol_avg = control_donations.mean()\n\n# Create the histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donations, bins=30, color='blue', alpha=0.7, edgecolor='black')\naxes[0].axvline(treatment_avg, color='red', linestyle='dashed', linewidth=2, label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group Donations')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donations, bins=30, color='green', alpha=0.7, edgecolor='black')\naxes[1].axvline(control_avg, color='red', linestyle='dashed', linewidth=2, label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group Donations')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/HW2/index copy.html#simulation-experiment",
    "href": "projects/HW2/index copy.html#simulation-experiment",
    "title": "Homework 2: A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set the probabilities for control and treatment groups\np_control = 0.018\np_treatment = 0.022\n\n# Simulate 10,000 draws from the Bernoulli distributions\ncontrol_draws = np.random.binomial(1, p_control, 10000)\ntreatment_draws = np.random.binomial(1, p_treatment, 10000)\n\n# Calculate the vector of differences\ndifferences = treatment_draws - control_draws\n\n# Calculate the cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences', color='blue')\nplt.axhline(0, color='red', linestyle='--', label='Zero Line')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average')\nplt.title('Cumulative Average of Differences Between Treatment and Control')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe chart illustrates the cumulative average of the differences between the treatment and control groups over 10,000 simulations. Initially, when only a few numbers are averaged, the cumulative average is “noisy” and fluctuates significantly due to the small sample size. However, as the sample size increases, the cumulative average begins to stabilize and converge toward the true treatment effect, which is approximately 0.004 (calculated as 0.022 - 0.018). This behavior demonstrates the law of large numbers, where larger sample sizes reduce variability and provide a more accurate estimate of the true effect. The red dashed line at zero serves as a reference point, highlighting the positive treatment effect as the cumulative average settles above this line.\n\n\nCentral Limit Theorem\n\n# Set the probabilities for control and treatment groups\np_control = 0.018\np_treatment = 0.022\n\n# Define sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\n# Generate histograms for each sample size\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, p_control, sample_size)\n        treatment_sample = np.random.binomial(1, p_treatment, sample_size)\n        avg_difference = treatment_sample.mean() - control_sample.mean()\n        avg_differences.append(avg_difference)\n    \n    # Plot the histogram\n    axes[i].hist(avg_differences, bins=30, color='blue', alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n\n(array([  1.,   0.,   0.,   0.,   4.,   0.,   0.,  24.,   0.,  59.,  15.,\n         0., 182.,   0.,   0., 319.,   0., 232.,   0., 112.,   0.,   0.,\n        41.,   0.,   9.,   0.,   0.,   1.,   0.,   1.]), array([-0.12 , -0.112, -0.104, -0.096, -0.088, -0.08 , -0.072, -0.064,\n       -0.056, -0.048, -0.04 , -0.032, -0.024, -0.016, -0.008,  0.   ,\n        0.008,  0.016,  0.024,  0.032,  0.04 ,  0.048,  0.056,  0.064,\n        0.072,  0.08 ,  0.088,  0.096,  0.104,  0.112,  0.12 ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 50')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  3.,   2.,   0.,  10.,   0.,  14.,  28.,   0.,  50.,   0.,  81.,\n       118.,   0., 131.,   0., 147., 135.,   0., 106.,   0.,  83.,  46.,\n         0.,  27.,   0.,   8.,   6.,   0.,   4.,   1.]), array([-0.04 , -0.037, -0.034, -0.031, -0.028, -0.025, -0.022, -0.019,\n       -0.016, -0.013, -0.01 , -0.007, -0.004, -0.001,  0.002,  0.005,\n        0.008,  0.011,  0.014,  0.017,  0.02 ,  0.023,  0.026,  0.029,\n        0.032,  0.035,  0.038,  0.041,  0.044,  0.047,  0.05 ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 200')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  1.,   0.,   0.,   2.,   7.,   8.,   9.,  13.,  38.,  40.,  44.,\n        60.,  69.,  84.,  76., 100.,  92.,  90.,  73.,  65.,  45.,  28.,\n        17.,  13.,  12.,   6.,   5.,   2.,   0.,   1.]), array([-0.026     , -0.02406667, -0.02213333, -0.0202    , -0.01826667,\n       -0.01633333, -0.0144    , -0.01246667, -0.01053333, -0.0086    ,\n       -0.00666667, -0.00473333, -0.0028    , -0.00086667,  0.00106667,\n        0.003     ,  0.00493333,  0.00686667,  0.0088    ,  0.01073333,\n        0.01266667,  0.0146    ,  0.01653333,  0.01846667,  0.0204    ,\n        0.02233333,  0.02426667,  0.0262    ,  0.02813333,  0.03006667,\n        0.032     ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 500')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n(array([  2.,   1.,   2.,   2.,   4.,   9.,  14.,  12.,  36.,  26.,  60.,\n        51.,  51., 119.,  73.,  70., 125.,  56.,  56.,  75.,  39.,  52.,\n        16.,  18.,  18.,   8.,   4.,   0.,   0.,   1.]), array([-0.017     , -0.01563333, -0.01426667, -0.0129    , -0.01153333,\n       -0.01016667, -0.0088    , -0.00743333, -0.00606667, -0.0047    ,\n       -0.00333333, -0.00196667, -0.0006    ,  0.00076667,  0.00213333,\n        0.0035    ,  0.00486667,  0.00623333,  0.0076    ,  0.00896667,\n        0.01033333,  0.0117    ,  0.01306667,  0.01443333,  0.0158    ,\n        0.01716667,  0.01853333,  0.0199    ,  0.02126667,  0.02263333,\n        0.024     ]), &lt;BarContainer object of 30 artists&gt;)\nText(0.5, 1.0, 'Sample Size: 1000')\nText(0.5, 0, 'Average Difference')\nText(0, 0.5, 'Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms illustrate the Central Limit Theorem (CLT): as sample size increases (50, 200, 500, 1000), the sampling distribution of the sample mean becomes smoother and more symmetric, converging toward a normal distribution. Larger sample sizes reduce variability and provide more precise estimates of the true mean.\n\nx &lt;- 5\nprint(x)\n\n[1] 5"
  },
  {
    "objectID": "projects/Homework2/index copy.html",
    "href": "projects/Homework2/index copy.html",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\n\n# Load and read the files\nairbnb_data = pd.read_csv('/home/jovyan/mysite/airbnb.csv')\nblueprinty_data = pd.read_csv('/home/jovyan/mysite/blueprinty.csv')\n\n# Display the first few rows of the dataset\nprint(airbnb_data.head())\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\n\n# Group the data by customer status and calculate the mean number of patents\nmeans = blueprinty_data.groupby('iscustomer')['patents'].mean()\nprint(means)\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n# Plot histograms for each customer status\nstatuses = blueprinty_data['iscustomer'].unique()\nfor status in statuses:\n    subset = blueprinty_data[blueprinty_data['iscustomer'] == status]\n    plt.hist(subset['patents'], bins=10, alpha=0.5, label=f'Status: {status}')\n    if status == 0:\n        plt.axvline(means[status], color='blue', linestyle='dashed', linewidth=1, label=f'Mean for {status}')\n    else:\n        plt.axvline(means[status], color='green', linestyle='dotted', linewidth=1, label=f'Mean for {status}')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histograms of Number of Patents by Customer Status')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram reveals that companies who are customers of the software service tend to have a slightly higher mean number of patents (4.13) compared to companies who are not customers (3.47). This suggests that being a customer of the software service may correlate with a higher number of patents, though further analysis would be needed to establish causation.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Group by customer status and analyze region distribution\nregion_distribution = blueprinty_data.groupby(['iscustomer', 'region']).size().unstack(fill_value=0)\n\n# Group by customer status and calculate mean age\nage_mean = blueprinty_data.groupby('iscustomer')['age'].mean()\n\nprint(\"Region Distribution by Customer Status:\")\nprint(region_distribution)\n\nprint(\"\\nMean Age by Customer Status:\")\nprint(age_mean)\n\nRegion Distribution by Customer Status:\nregion      Midwest  Northeast  Northwest  South  Southwest\niscustomer                                                 \n0               187        273        158    156        245\n1                37        328         29     35         52\n\nMean Age by Customer Status:\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nObservations: 1. The Northeast region has the highest number of customers (328) compared to other regions. 2. The Midwest region has the lowest number of customers (37). 3. Non-customers are more evenly distributed across regions, with the Southwest region having the highest count (245) and the Northwest region having the lowest count (158). 4. Customers are concentrated more in the Northeast region, while non-customers are more prevalent in the Southwest region. 5. The South region has a relatively low number of both customers (35) and non-customers (156).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nfrom sympy import symbols, factorial, exp, prod\n\n# Define variables\nY, lam = symbols('Y lambda', positive=True, integer=True)\n\n# Poisson probability mass function\npoisson_pmf = (exp(-lam) * lam**Y) / factorial(Y)\n\n# Likelihood for a dataset of observations\nobservations = symbols('Y1 Y2 Y3 Y4', positive=True, integer=True)  # Example observations\nlikelihood = prod((exp(-lam) * lam**obs) / factorial(obs) for obs in observations)\n\nprint(\"Poisson PMF:\", poisson_pmf)\nprint(\"Likelihood for observations:\", likelihood)\n\nPoisson PMF: lambda**Y*exp(-lambda)/factorial(Y)\nLikelihood for observations: lambda**Y1*lambda**Y2*lambda**Y3*lambda**Y4*exp(-4*lambda)/(factorial(Y1)*factorial(Y2)*factorial(Y3)*factorial(Y4))\n\n\n\nfrom sympy import log, lambdify\n\n# Define the log-likelihood function\ndef poisson_loglikelihood(lam, observations):\n    log_likelihood = sum(log((exp(-lam) * lam**obs) / factorial(obs)) for obs in observations)\n    return log_likelihood\n\n# Example usage\nlog_likelihood_expr = poisson_loglikelihood(lam, observations)\nprint(\"Log-Likelihood Expression:\", log_likelihood_expr)\n\n# Optionally, convert to a numerical function\nlog_likelihood_func = lambdify((lam, observations), log_likelihood_expr)\n\nLog-Likelihood Expression: log(lambda**Y1*exp(-lambda)/factorial(Y1)) + log(lambda**Y2*exp(-lambda)/factorial(Y2)) + log(lambda**Y3*exp(-lambda)/factorial(Y3)) + log(lambda**Y4*exp(-lambda)/factorial(Y4))\n\n\n\n# Extract the observed number of patents\nobserved_patents = blueprinty_data['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 10, 100)\n\n# Compute the log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lam_val, observed_patents) for lam_val in lambda_values]\n\n# Plot the log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood', color='blue')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood vs Lambda')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import minimize\n\n# Define a numerical version of the log-likelihood function\ndef numerical_log_likelihood(lam_val):\n    return -sum(np.log((np.exp(-lam_val) * lam_val**obs) / np.math.factorial(obs)) for obs in observed_patents)\n\n# Initial guess for lambda\ninitial_guess = 1.0\n\n# Perform the optimization\nresult = minimize(numerical_log_likelihood, initial_guess, bounds=[(0.1, None)])\n\n# Extract the MLE for lambda\nmle_lambda = result.x[0]\nprint(\"MLE for lambda:\", mle_lambda)\n\n/tmp/ipykernel_50603/2240072717.py:5: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\nMLE for lambda: 3.684670911307255\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nfrom sympy import Matrix, exp, symbols\n\n# Define the Poisson regression likelihood function\ndef poisson_regression_likelihood(beta, Y, X):\n    # Convert beta and X to symbolic matrices\n    beta = Matrix(beta)\n    X = Matrix(X)\n    \n    # Compute lambda_i = exp(X_i' * beta) for each observation\n    lambdas = (X * beta).applyfunc(exp)\n    \n    # Compute the likelihood\n    likelihood = prod((lambdas[i]**Y[i] * exp(-lambdas[i])) / factorial(Y[i]) for i in range(len(Y)))\n    return likelihood\n\n# Example usage\nbeta = symbols('beta0 beta1', real=True)  # Example beta vector\nX = [[1, 2], [1, 3], [1, 4]]  # Example covariate matrix\nY = [2, 3, 4]  # Example observations\n\nlikelihood = poisson_regression_likelihood(beta, Y, X)\nprint(\"Poisson Regression Likelihood:\", likelihood)\n\nPoisson Regression Likelihood: exp(2*beta0 + 4*beta1)*exp(3*beta0 + 9*beta1)*exp(4*beta0 + 16*beta1)*exp(-exp(beta0 + 2*beta1))*exp(-exp(beta0 + 3*beta1))*exp(-exp(beta0 + 4*beta1))/288\n\n\nIdentify the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\nfrom sympy import exp\n\n# Prepare the covariate matrix X\nblueprinty_data['age_squared'] = blueprinty_data['age'] ** 2\nregion_dummies = pd.get_dummies(blueprinty_data['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=blueprinty_data.index, name='Intercept'),\n               blueprinty_data['age'], blueprinty_data['age_squared'],\n               region_dummies, blueprinty_data['iscustomer']], axis=1).astype(float).values\n\n# Response variable Y\nY = blueprinty_data['patents'].values\n\n# Define the Poisson regression log-likelihood function\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lambdas = np.exp(np.dot(X, beta))\n    log_likelihood = np.sum(Y * np.log(lambdas) - lambdas - np.log(np.array([np.math.factorial(y) for y in Y])))\n    return -log_likelihood  # Negative for minimization\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Perform the optimization\nresult = minimize(poisson_regression_loglikelihood, initial_beta, args=(Y, X), method='BFGS')\n\n# Extract the MLE for beta and the Hessian\nmle_beta = result.x\nhessian_inv = result.hess_inv\n\n# Calculate standard errors from the Hessian\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create a table of coefficients and standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': mle_beta,\n    'Standard Error': standard_errors\n}, index=['Intercept', 'Age', 'Age Squared'] + list(region_dummies.columns) + ['Is Customer'])\n\nprint(coefficients_table)\n\n/tmp/ipykernel_50603/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/tmp/ipykernel_50603/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n\n\n             Coefficient  Standard Error\nIntercept       1.480059             1.0\nAge            38.016417             1.0\nAge Squared  1033.539585             1.0\nNortheast       0.640979             1.0\nNorthwest       0.164288             1.0\nSouth           0.181562             1.0\nSouthwest       0.295497             1.0\nIs Customer     0.553874             1.0\n\n\n/tmp/ipykernel_50603/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_50603/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n/tmp/ipykernel_50603/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_50603/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using GLM\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Print the summary of the model\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 12 May 2025   Deviance:                       2143.3\nTime:                        14:24:31   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nx1             0.1486      0.014     10.716      0.000       0.121       0.176\nx2            -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe results of the generalized linear regression model can be interpreted as follows:\n\nIntercept (const): The coefficient for the intercept is -0.5089, which represents the baseline log-expected value of the dependent variable (y) when all predictors are zero. This value is statistically significant (p-value = 0.005).\nPredictor x1: The coefficient for x1 is 0.1486, indicating that for a one-unit increase in x1, the expected value of y increases by approximately 14.86% (since the link function is log, the effect is multiplicative). This is highly significant (p-value &lt; 0.001).\nPredictor x2: The coefficient for x2 is -0.0030, suggesting that a one-unit increase in x2 decreases the expected value of y by approximately 0.3%. This is also highly significant (p-value &lt; 0.001).\nPredictors x3, x4, x5, and x6: These predictors have coefficients close to zero and high p-values (greater than 0.05), indicating that they are not statistically significant in explaining the variation in y.\nPredictor x7: The coefficient for x7 is 0.2076, meaning that a one-unit increase in x7 increases the expected value of y by approximately 20.76%. This is statistically significant (p-value &lt; 0.001).\nModel Fit:\n\nLog-Likelihood: The log-likelihood value is -3258.1, which reflects the fit of the model to the data.\nDeviance: The deviance is 2143.3, which measures the goodness of fit. Lower values indicate a better fit.\nPseudo R-squared (CS): The pseudo R-squared value is 0.1360, suggesting that the model explains about 13.6% of the variability in the dependent variable.\n\nSignificance: Predictors x1, x2, and x7 are statistically significant, while the others are not. This suggests that these three variables are the most important in explaining the variation in y.\n\nIn summary, the model identifies x1, x2, and x7 as significant predictors of the dependent variable, while the other predictors do not contribute significantly. The overall fit of the model is moderate, as indicated by the pseudo R-squared value."
  },
  {
    "objectID": "projects/Homework2/index copy.html#blueprinty-case-study",
    "href": "projects/Homework2/index copy.html#blueprinty-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\n\n# Load and read the files\nairbnb_data = pd.read_csv('/home/jovyan/mysite/airbnb.csv')\nblueprinty_data = pd.read_csv('/home/jovyan/mysite/blueprinty.csv')\n\n# Display the first few rows of the dataset\nprint(airbnb_data.head())\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\n\n# Group the data by customer status and calculate the mean number of patents\nmeans = blueprinty_data.groupby('iscustomer')['patents'].mean()\nprint(means)\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n# Plot histograms for each customer status\nstatuses = blueprinty_data['iscustomer'].unique()\nfor status in statuses:\n    subset = blueprinty_data[blueprinty_data['iscustomer'] == status]\n    plt.hist(subset['patents'], bins=10, alpha=0.5, label=f'Status: {status}')\n    if status == 0:\n        plt.axvline(means[status], color='blue', linestyle='dashed', linewidth=1, label=f'Mean for {status}')\n    else:\n        plt.axvline(means[status], color='green', linestyle='dotted', linewidth=1, label=f'Mean for {status}')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histograms of Number of Patents by Customer Status')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram reveals that companies who are customers of the software service tend to have a slightly higher mean number of patents (4.13) compared to companies who are not customers (3.47). This suggests that being a customer of the software service may correlate with a higher number of patents, though further analysis would be needed to establish causation.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Group by customer status and analyze region distribution\nregion_distribution = blueprinty_data.groupby(['iscustomer', 'region']).size().unstack(fill_value=0)\n\n# Group by customer status and calculate mean age\nage_mean = blueprinty_data.groupby('iscustomer')['age'].mean()\n\nprint(\"Region Distribution by Customer Status:\")\nprint(region_distribution)\n\nprint(\"\\nMean Age by Customer Status:\")\nprint(age_mean)\n\nRegion Distribution by Customer Status:\nregion      Midwest  Northeast  Northwest  South  Southwest\niscustomer                                                 \n0               187        273        158    156        245\n1                37        328         29     35         52\n\nMean Age by Customer Status:\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nObservations: 1. The Northeast region has the highest number of customers (328) compared to other regions. 2. The Midwest region has the lowest number of customers (37). 3. Non-customers are more evenly distributed across regions, with the Southwest region having the highest count (245) and the Northwest region having the lowest count (158). 4. Customers are concentrated more in the Northeast region, while non-customers are more prevalent in the Southwest region. 5. The South region has a relatively low number of both customers (35) and non-customers (156).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nfrom sympy import symbols, factorial, exp, prod\n\n# Define variables\nY, lam = symbols('Y lambda', positive=True, integer=True)\n\n# Poisson probability mass function\npoisson_pmf = (exp(-lam) * lam**Y) / factorial(Y)\n\n# Likelihood for a dataset of observations\nobservations = symbols('Y1 Y2 Y3 Y4', positive=True, integer=True)  # Example observations\nlikelihood = prod((exp(-lam) * lam**obs) / factorial(obs) for obs in observations)\n\nprint(\"Poisson PMF:\", poisson_pmf)\nprint(\"Likelihood for observations:\", likelihood)\n\nPoisson PMF: lambda**Y*exp(-lambda)/factorial(Y)\nLikelihood for observations: lambda**Y1*lambda**Y2*lambda**Y3*lambda**Y4*exp(-4*lambda)/(factorial(Y1)*factorial(Y2)*factorial(Y3)*factorial(Y4))\n\n\n\nfrom sympy import log, lambdify\n\n# Define the log-likelihood function\ndef poisson_loglikelihood(lam, observations):\n    log_likelihood = sum(log((exp(-lam) * lam**obs) / factorial(obs)) for obs in observations)\n    return log_likelihood\n\n# Example usage\nlog_likelihood_expr = poisson_loglikelihood(lam, observations)\nprint(\"Log-Likelihood Expression:\", log_likelihood_expr)\n\n# Optionally, convert to a numerical function\nlog_likelihood_func = lambdify((lam, observations), log_likelihood_expr)\n\nLog-Likelihood Expression: log(lambda**Y1*exp(-lambda)/factorial(Y1)) + log(lambda**Y2*exp(-lambda)/factorial(Y2)) + log(lambda**Y3*exp(-lambda)/factorial(Y3)) + log(lambda**Y4*exp(-lambda)/factorial(Y4))\n\n\n\n# Extract the observed number of patents\nobserved_patents = blueprinty_data['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 10, 100)\n\n# Compute the log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lam_val, observed_patents) for lam_val in lambda_values]\n\n# Plot the log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood', color='blue')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood vs Lambda')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import minimize\n\n# Define a numerical version of the log-likelihood function\ndef numerical_log_likelihood(lam_val):\n    return -sum(np.log((np.exp(-lam_val) * lam_val**obs) / np.math.factorial(obs)) for obs in observed_patents)\n\n# Initial guess for lambda\ninitial_guess = 1.0\n\n# Perform the optimization\nresult = minimize(numerical_log_likelihood, initial_guess, bounds=[(0.1, None)])\n\n# Extract the MLE for lambda\nmle_lambda = result.x[0]\nprint(\"MLE for lambda:\", mle_lambda)\n\n/tmp/ipykernel_50603/2240072717.py:5: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\nMLE for lambda: 3.684670911307255\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nfrom sympy import Matrix, exp, symbols\n\n# Define the Poisson regression likelihood function\ndef poisson_regression_likelihood(beta, Y, X):\n    # Convert beta and X to symbolic matrices\n    beta = Matrix(beta)\n    X = Matrix(X)\n    \n    # Compute lambda_i = exp(X_i' * beta) for each observation\n    lambdas = (X * beta).applyfunc(exp)\n    \n    # Compute the likelihood\n    likelihood = prod((lambdas[i]**Y[i] * exp(-lambdas[i])) / factorial(Y[i]) for i in range(len(Y)))\n    return likelihood\n\n# Example usage\nbeta = symbols('beta0 beta1', real=True)  # Example beta vector\nX = [[1, 2], [1, 3], [1, 4]]  # Example covariate matrix\nY = [2, 3, 4]  # Example observations\n\nlikelihood = poisson_regression_likelihood(beta, Y, X)\nprint(\"Poisson Regression Likelihood:\", likelihood)\n\nPoisson Regression Likelihood: exp(2*beta0 + 4*beta1)*exp(3*beta0 + 9*beta1)*exp(4*beta0 + 16*beta1)*exp(-exp(beta0 + 2*beta1))*exp(-exp(beta0 + 3*beta1))*exp(-exp(beta0 + 4*beta1))/288\n\n\nIdentify the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\nfrom sympy import exp\n\n# Prepare the covariate matrix X\nblueprinty_data['age_squared'] = blueprinty_data['age'] ** 2\nregion_dummies = pd.get_dummies(blueprinty_data['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=blueprinty_data.index, name='Intercept'),\n               blueprinty_data['age'], blueprinty_data['age_squared'],\n               region_dummies, blueprinty_data['iscustomer']], axis=1).astype(float).values\n\n# Response variable Y\nY = blueprinty_data['patents'].values\n\n# Define the Poisson regression log-likelihood function\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lambdas = np.exp(np.dot(X, beta))\n    log_likelihood = np.sum(Y * np.log(lambdas) - lambdas - np.log(np.array([np.math.factorial(y) for y in Y])))\n    return -log_likelihood  # Negative for minimization\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Perform the optimization\nresult = minimize(poisson_regression_loglikelihood, initial_beta, args=(Y, X), method='BFGS')\n\n# Extract the MLE for beta and the Hessian\nmle_beta = result.x\nhessian_inv = result.hess_inv\n\n# Calculate standard errors from the Hessian\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create a table of coefficients and standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': mle_beta,\n    'Standard Error': standard_errors\n}, index=['Intercept', 'Age', 'Age Squared'] + list(region_dummies.columns) + ['Is Customer'])\n\nprint(coefficients_table)\n\n/tmp/ipykernel_50603/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/tmp/ipykernel_50603/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n\n\n             Coefficient  Standard Error\nIntercept       1.480059             1.0\nAge            38.016417             1.0\nAge Squared  1033.539585             1.0\nNortheast       0.640979             1.0\nNorthwest       0.164288             1.0\nSouth           0.181562             1.0\nSouthwest       0.295497             1.0\nIs Customer     0.553874             1.0\n\n\n/tmp/ipykernel_50603/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_50603/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n/tmp/ipykernel_50603/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_50603/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_50603/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using GLM\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Print the summary of the model\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 12 May 2025   Deviance:                       2143.3\nTime:                        14:24:31   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nx1             0.1486      0.014     10.716      0.000       0.121       0.176\nx2            -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe results of the generalized linear regression model can be interpreted as follows:\n\nIntercept (const): The coefficient for the intercept is -0.5089, which represents the baseline log-expected value of the dependent variable (y) when all predictors are zero. This value is statistically significant (p-value = 0.005).\nPredictor x1: The coefficient for x1 is 0.1486, indicating that for a one-unit increase in x1, the expected value of y increases by approximately 14.86% (since the link function is log, the effect is multiplicative). This is highly significant (p-value &lt; 0.001).\nPredictor x2: The coefficient for x2 is -0.0030, suggesting that a one-unit increase in x2 decreases the expected value of y by approximately 0.3%. This is also highly significant (p-value &lt; 0.001).\nPredictors x3, x4, x5, and x6: These predictors have coefficients close to zero and high p-values (greater than 0.05), indicating that they are not statistically significant in explaining the variation in y.\nPredictor x7: The coefficient for x7 is 0.2076, meaning that a one-unit increase in x7 increases the expected value of y by approximately 20.76%. This is statistically significant (p-value &lt; 0.001).\nModel Fit:\n\nLog-Likelihood: The log-likelihood value is -3258.1, which reflects the fit of the model to the data.\nDeviance: The deviance is 2143.3, which measures the goodness of fit. Lower values indicate a better fit.\nPseudo R-squared (CS): The pseudo R-squared value is 0.1360, suggesting that the model explains about 13.6% of the variability in the dependent variable.\n\nSignificance: Predictors x1, x2, and x7 are statistically significant, while the others are not. This suggests that these three variables are the most important in explaining the variation in y.\n\nIn summary, the model identifies x1, x2, and x7 as significant predictors of the dependent variable, while the other predictors do not contribute significantly. The overall fit of the model is moderate, as indicated by the pseudo R-squared value."
  },
  {
    "objectID": "projects/Homework2/index copy.html#airbnb-case-study",
    "href": "projects/Homework2/index copy.html#airbnb-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Drop observations with missing values in relevant columns\nrelevant_columns = ['number_of_reviews', 'price', 'room_type', 'bathrooms', 'bedrooms']\ncleaned_airbnb_data = airbnb_data.dropna(subset=relevant_columns)\n\n# Summary statistics for the number of reviews\nprint(\"Summary statistics for the number of reviews:\")\nprint(cleaned_airbnb_data['number_of_reviews'].describe())\n\n# Distribution of the number of reviews\nplt.figure(figsize=(10, 6))\nplt.hist(cleaned_airbnb_data['number_of_reviews'], bins=50, color='skyblue', edgecolor='black')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.title('Distribution of Number of Reviews')\nplt.grid(axis='y')\nplt.show()\n\n# Relationship between price and number of reviews\nplt.figure(figsize=(10, 6))\nplt.scatter(cleaned_airbnb_data['price'], cleaned_airbnb_data['number_of_reviews'], alpha=0.5, color='purple')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.title('Price vs Number of Reviews')\nplt.grid()\nplt.show()\n\n# Average number of reviews by room type\navg_reviews_by_room_type = cleaned_airbnb_data.groupby('room_type')['number_of_reviews'].mean()\nprint(\"\\nAverage number of reviews by room type:\")\nprint(avg_reviews_by_room_type)\n\n# Bar plot for average number of reviews by room type\navg_reviews_by_room_type.plot(kind='bar', color='orange', edgecolor='black', figsize=(8, 5))\nplt.xlabel('Room Type')\nplt.ylabel('Average Number of Reviews')\nplt.title('Average Number of Reviews by Room Type')\nplt.grid(axis='y')\nplt.show()\n\nSummary statistics for the number of reviews:\ncount    40395.000000\nmean        15.837455\nstd         29.138007\nmin          0.000000\n25%          1.000000\n50%          4.000000\n75%         17.000000\nmax        421.000000\nName: number_of_reviews, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage number of reviews by room type:\nroom_type\nEntire home/apt    16.805713\nPrivate room       15.103985\nShared room        11.793924\nName: number_of_reviews, dtype: float64\n\n\n\n\n\n\n\n\n\n\n# Prepare the covariate matrix X for the Airbnb data\n# Include relevant variables such as price, room_type (encoded as dummies), bathrooms, and bedrooms\nroom_type_dummies = pd.get_dummies(cleaned_airbnb_data['room_type'], drop_first=True)\nX_airbnb = pd.concat([cleaned_airbnb_data[['price', 'bathrooms', 'bedrooms']], room_type_dummies], axis=1)\n\n# Convert boolean columns to integers\nX_airbnb = X_airbnb.astype(float)\n\n# Add a constant term for the intercept\nX_airbnb = sm.add_constant(X_airbnb)\n\n# Response variable Y (number of reviews)\nY_airbnb = cleaned_airbnb_data['number_of_reviews']\n\n# Fit a Poisson regression model\npoisson_model_airbnb = sm.GLM(Y_airbnb, X_airbnb, family=sm.families.Poisson())\npoisson_results_airbnb = poisson_model_airbnb.fit()\n\n# Print the summary of the model\nprint(poisson_results_airbnb.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                40395\nModel:                            GLM   Df Residuals:                    40389\nModel Family:                 Poisson   Df Model:                            5\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -7.2165e+05\nDate:                Mon, 12 May 2025   Deviance:                   1.3201e+06\nTime:                        14:24:32   Pearson chi2:                 2.18e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.1747\nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            2.9065      0.004    661.204      0.000       2.898       2.915\nprice           -0.0005   1.24e-05    -40.928      0.000      -0.001      -0.000\nbathrooms       -0.1052      0.004    -27.149      0.000      -0.113      -0.098\nbedrooms         0.1042      0.002     51.830      0.000       0.100       0.108\nPrivate room    -0.1398      0.003    -49.040      0.000      -0.145      -0.134\nShared room     -0.3895      0.009    -44.998      0.000      -0.406      -0.373\n================================================================================\n\n\nThe coefficients from the Poisson regression model can be interpreted as follows:\n\nIntercept (const): The baseline log-expected number of reviews is approximately 2.9065 when all other variables are zero. This corresponds to an expected number of reviews of ( e^{2.9065} ).\nPrice (price): For every one-unit increase in price, the expected number of reviews decreases by approximately ( e^{-0.0005} - 1 % ). This indicates a very small negative relationship between price and the number of reviews.\nBathrooms (bathrooms): For every additional bathroom, the expected number of reviews decreases by approximately ( e^{-0.1052} - 1 % ). This suggests that listings with more bathrooms tend to have fewer reviews.\nBedrooms (bedrooms): For every additional bedroom, the expected number of reviews increases by approximately ( e^{0.1042} - 1 % ). This indicates that listings with more bedrooms tend to have more reviews.\nPrivate Room (Private room): Compared to the baseline category (Entire home/apt), private rooms have an expected number of reviews that is approximately ( e^{-0.1398} - 1 % ) lower.\nShared Room (Shared room): Compared to the baseline category (Entire home/apt), shared rooms have an expected number of reviews that is approximately ( e^{-0.3895} - 1 % ) lower.\n\n\n\nSummary:\n\nListings with higher prices and more bathrooms tend to have fewer reviews.\nListings with more bedrooms tend to have more reviews.\nPrivate and shared rooms receive fewer reviews compared to entire homes/apartments."
  },
  {
    "objectID": "projects/Homework2/index.html",
    "href": "projects/Homework2/index.html",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\n\n# Load and read the files\nairbnb_data = pd.read_csv('/home/jovyan/mysite/airbnb.csv')\nblueprinty_data = pd.read_csv('/home/jovyan/mysite/blueprinty.csv')\n\n# Display the first few rows of the dataset\nprint(airbnb_data.head())\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\n\n# Group the data by customer status and calculate the mean number of patents\nmeans = blueprinty_data.groupby('iscustomer')['patents'].mean()\nprint(means)\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n# Plot histograms for each customer status\nstatuses = blueprinty_data['iscustomer'].unique()\nfor status in statuses:\n    subset = blueprinty_data[blueprinty_data['iscustomer'] == status]\n    plt.hist(subset['patents'], bins=10, alpha=0.5, label=f'Status: {status}')\n    if status == 0:\n        plt.axvline(means[status], color='blue', linestyle='dashed', linewidth=1, label=f'Mean for {status}')\n    else:\n        plt.axvline(means[status], color='green', linestyle='dotted', linewidth=1, label=f'Mean for {status}')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histograms of Number of Patents by Customer Status')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram reveals that companies who are customers of the software service tend to have a slightly higher mean number of patents (4.13) compared to companies who are not customers (3.47). This suggests that being a customer of the software service may correlate with a higher number of patents, though further analysis would be needed to establish causation.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Group by customer status and analyze region distribution\nregion_distribution = blueprinty_data.groupby(['iscustomer', 'region']).size().unstack(fill_value=0)\n\n# Group by customer status and calculate mean age\nage_mean = blueprinty_data.groupby('iscustomer')['age'].mean()\n\nprint(\"Region Distribution by Customer Status:\")\nprint(region_distribution)\n\nprint(\"\\nMean Age by Customer Status:\")\nprint(age_mean)\n\nRegion Distribution by Customer Status:\nregion      Midwest  Northeast  Northwest  South  Southwest\niscustomer                                                 \n0               187        273        158    156        245\n1                37        328         29     35         52\n\nMean Age by Customer Status:\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nObservations: 1. The Northeast region has the highest number of customers (328) compared to other regions. 2. The Midwest region has the lowest number of customers (37). 3. Non-customers are more evenly distributed across regions, with the Southwest region having the highest count (245) and the Northwest region having the lowest count (158). 4. Customers are concentrated more in the Northeast region, while non-customers are more prevalent in the Southwest region. 5. The South region has a relatively low number of both customers (35) and non-customers (156).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nfrom sympy import symbols, factorial, exp, prod\n\n# Define variables\nY, lam = symbols('Y lambda', positive=True, integer=True)\n\n# Poisson probability mass function\npoisson_pmf = (exp(-lam) * lam**Y) / factorial(Y)\n\n# Likelihood for a dataset of observations\nobservations = symbols('Y1 Y2 Y3 Y4', positive=True, integer=True)  # Example observations\nlikelihood = prod((exp(-lam) * lam**obs) / factorial(obs) for obs in observations)\n\nprint(\"Poisson PMF:\", poisson_pmf)\nprint(\"Likelihood for observations:\", likelihood)\n\nPoisson PMF: lambda**Y*exp(-lambda)/factorial(Y)\nLikelihood for observations: lambda**Y1*lambda**Y2*lambda**Y3*lambda**Y4*exp(-4*lambda)/(factorial(Y1)*factorial(Y2)*factorial(Y3)*factorial(Y4))\n\n\n\nfrom sympy import log, lambdify\n\n# Define the log-likelihood function\ndef poisson_loglikelihood(lam, observations):\n    log_likelihood = sum(log((exp(-lam) * lam**obs) / factorial(obs)) for obs in observations)\n    return log_likelihood\n\n# Example usage\nlog_likelihood_expr = poisson_loglikelihood(lam, observations)\nprint(\"Log-Likelihood Expression:\", log_likelihood_expr)\n\n# Optionally, convert to a numerical function\nlog_likelihood_func = lambdify((lam, observations), log_likelihood_expr)\n\nLog-Likelihood Expression: log(lambda**Y1*exp(-lambda)/factorial(Y1)) + log(lambda**Y2*exp(-lambda)/factorial(Y2)) + log(lambda**Y3*exp(-lambda)/factorial(Y3)) + log(lambda**Y4*exp(-lambda)/factorial(Y4))\n\n\n\n# Extract the observed number of patents\nobserved_patents = blueprinty_data['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 10, 100)\n\n# Compute the log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lam_val, observed_patents) for lam_val in lambda_values]\n\n# Plot the log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood', color='blue')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood vs Lambda')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import minimize\n\n# Define a numerical version of the log-likelihood function\ndef numerical_log_likelihood(lam_val):\n    return -sum(np.log((np.exp(-lam_val) * lam_val**obs) / np.math.factorial(obs)) for obs in observed_patents)\n\n# Initial guess for lambda\ninitial_guess = 1.0\n\n# Perform the optimization\nresult = minimize(numerical_log_likelihood, initial_guess, bounds=[(0.1, None)])\n\n# Extract the MLE for lambda\nmle_lambda = result.x[0]\nprint(\"MLE for lambda:\", mle_lambda)\n\n/tmp/ipykernel_52268/2240072717.py:5: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\nMLE for lambda: 3.684670911307255\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nfrom sympy import Matrix, exp, symbols\n\n# Define the Poisson regression likelihood function\ndef poisson_regression_likelihood(beta, Y, X):\n    # Convert beta and X to symbolic matrices\n    beta = Matrix(beta)\n    X = Matrix(X)\n    \n    # Compute lambda_i = exp(X_i' * beta) for each observation\n    lambdas = (X * beta).applyfunc(exp)\n    \n    # Compute the likelihood\n    likelihood = prod((lambdas[i]**Y[i] * exp(-lambdas[i])) / factorial(Y[i]) for i in range(len(Y)))\n    return likelihood\n\n# Example usage\nbeta = symbols('beta0 beta1', real=True)  # Example beta vector\nX = [[1, 2], [1, 3], [1, 4]]  # Example covariate matrix\nY = [2, 3, 4]  # Example observations\n\nlikelihood = poisson_regression_likelihood(beta, Y, X)\nprint(\"Poisson Regression Likelihood:\", likelihood)\n\nPoisson Regression Likelihood: exp(2*beta0 + 4*beta1)*exp(3*beta0 + 9*beta1)*exp(4*beta0 + 16*beta1)*exp(-exp(beta0 + 2*beta1))*exp(-exp(beta0 + 3*beta1))*exp(-exp(beta0 + 4*beta1))/288\n\n\nIdentify the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\nfrom sympy import exp\n\n# Prepare the covariate matrix X\nblueprinty_data['age_squared'] = blueprinty_data['age'] ** 2\nregion_dummies = pd.get_dummies(blueprinty_data['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=blueprinty_data.index, name='Intercept'),\n               blueprinty_data['age'], blueprinty_data['age_squared'],\n               region_dummies, blueprinty_data['iscustomer']], axis=1).astype(float).values\n\n# Response variable Y\nY = blueprinty_data['patents'].values\n\n# Define the Poisson regression log-likelihood function\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lambdas = np.exp(np.dot(X, beta))\n    log_likelihood = np.sum(Y * np.log(lambdas) - lambdas - np.log(np.array([np.math.factorial(y) for y in Y])))\n    return -log_likelihood  # Negative for minimization\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Perform the optimization\nresult = minimize(poisson_regression_loglikelihood, initial_beta, args=(Y, X), method='BFGS')\n\n# Extract the MLE for beta and the Hessian\nmle_beta = result.x\nhessian_inv = result.hess_inv\n\n# Calculate standard errors from the Hessian\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create a table of coefficients and standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': mle_beta,\n    'Standard Error': standard_errors\n}, index=['Intercept', 'Age', 'Age Squared'] + list(region_dummies.columns) + ['Is Customer'])\n\nprint(coefficients_table)\n\n/tmp/ipykernel_52268/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/tmp/ipykernel_52268/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n\n\n             Coefficient  Standard Error\nIntercept       1.480059             1.0\nAge            38.016417             1.0\nAge Squared  1033.539585             1.0\nNortheast       0.640979             1.0\nNorthwest       0.164288             1.0\nSouth           0.181562             1.0\nSouthwest       0.295497             1.0\nIs Customer     0.553874             1.0\n\n\n/tmp/ipykernel_52268/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_52268/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n/tmp/ipykernel_52268/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_52268/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using GLM\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Print the summary of the model\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 12 May 2025   Deviance:                       2143.3\nTime:                        14:25:21   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nx1             0.1486      0.014     10.716      0.000       0.121       0.176\nx2            -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe results of the generalized linear regression model can be interpreted as follows:\n\nIntercept (const): The coefficient for the intercept is -0.5089, which represents the baseline log-expected value of the dependent variable (y) when all predictors are zero. This value is statistically significant (p-value = 0.005).\nPredictor x1: The coefficient for x1 is 0.1486, indicating that for a one-unit increase in x1, the expected value of y increases by approximately 14.86% (since the link function is log, the effect is multiplicative). This is highly significant (p-value &lt; 0.001).\nPredictor x2: The coefficient for x2 is -0.0030, suggesting that a one-unit increase in x2 decreases the expected value of y by approximately 0.3%. This is also highly significant (p-value &lt; 0.001).\nPredictors x3, x4, x5, and x6: These predictors have coefficients close to zero and high p-values (greater than 0.05), indicating that they are not statistically significant in explaining the variation in y.\nPredictor x7: The coefficient for x7 is 0.2076, meaning that a one-unit increase in x7 increases the expected value of y by approximately 20.76%. This is statistically significant (p-value &lt; 0.001).\nModel Fit:\n\nLog-Likelihood: The log-likelihood value is -3258.1, which reflects the fit of the model to the data.\nDeviance: The deviance is 2143.3, which measures the goodness of fit. Lower values indicate a better fit.\nPseudo R-squared (CS): The pseudo R-squared value is 0.1360, suggesting that the model explains about 13.6% of the variability in the dependent variable.\n\nSignificance: Predictors x1, x2, and x7 are statistically significant, while the others are not. This suggests that these three variables are the most important in explaining the variation in y.\n\nIn summary, the model identifies x1, x2, and x7 as significant predictors of the dependent variable, while the other predictors do not contribute significantly. The overall fit of the model is moderate, as indicated by the pseudo R-squared value."
  },
  {
    "objectID": "projects/Homework2/index.html#blueprinty-case-study",
    "href": "projects/Homework2/index.html#blueprinty-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\n\n# Load and read the files\nairbnb_data = pd.read_csv('/home/jovyan/mysite/airbnb.csv')\nblueprinty_data = pd.read_csv('/home/jovyan/mysite/blueprinty.csv')\n\n# Display the first few rows of the dataset\nprint(airbnb_data.head())\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\n\n# Group the data by customer status and calculate the mean number of patents\nmeans = blueprinty_data.groupby('iscustomer')['patents'].mean()\nprint(means)\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n# Plot histograms for each customer status\nstatuses = blueprinty_data['iscustomer'].unique()\nfor status in statuses:\n    subset = blueprinty_data[blueprinty_data['iscustomer'] == status]\n    plt.hist(subset['patents'], bins=10, alpha=0.5, label=f'Status: {status}')\n    if status == 0:\n        plt.axvline(means[status], color='blue', linestyle='dashed', linewidth=1, label=f'Mean for {status}')\n    else:\n        plt.axvline(means[status], color='green', linestyle='dotted', linewidth=1, label=f'Mean for {status}')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histograms of Number of Patents by Customer Status')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram reveals that companies who are customers of the software service tend to have a slightly higher mean number of patents (4.13) compared to companies who are not customers (3.47). This suggests that being a customer of the software service may correlate with a higher number of patents, though further analysis would be needed to establish causation.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Group by customer status and analyze region distribution\nregion_distribution = blueprinty_data.groupby(['iscustomer', 'region']).size().unstack(fill_value=0)\n\n# Group by customer status and calculate mean age\nage_mean = blueprinty_data.groupby('iscustomer')['age'].mean()\n\nprint(\"Region Distribution by Customer Status:\")\nprint(region_distribution)\n\nprint(\"\\nMean Age by Customer Status:\")\nprint(age_mean)\n\nRegion Distribution by Customer Status:\nregion      Midwest  Northeast  Northwest  South  Southwest\niscustomer                                                 \n0               187        273        158    156        245\n1                37        328         29     35         52\n\nMean Age by Customer Status:\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nObservations: 1. The Northeast region has the highest number of customers (328) compared to other regions. 2. The Midwest region has the lowest number of customers (37). 3. Non-customers are more evenly distributed across regions, with the Southwest region having the highest count (245) and the Northwest region having the lowest count (158). 4. Customers are concentrated more in the Northeast region, while non-customers are more prevalent in the Southwest region. 5. The South region has a relatively low number of both customers (35) and non-customers (156).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nfrom sympy import symbols, factorial, exp, prod\n\n# Define variables\nY, lam = symbols('Y lambda', positive=True, integer=True)\n\n# Poisson probability mass function\npoisson_pmf = (exp(-lam) * lam**Y) / factorial(Y)\n\n# Likelihood for a dataset of observations\nobservations = symbols('Y1 Y2 Y3 Y4', positive=True, integer=True)  # Example observations\nlikelihood = prod((exp(-lam) * lam**obs) / factorial(obs) for obs in observations)\n\nprint(\"Poisson PMF:\", poisson_pmf)\nprint(\"Likelihood for observations:\", likelihood)\n\nPoisson PMF: lambda**Y*exp(-lambda)/factorial(Y)\nLikelihood for observations: lambda**Y1*lambda**Y2*lambda**Y3*lambda**Y4*exp(-4*lambda)/(factorial(Y1)*factorial(Y2)*factorial(Y3)*factorial(Y4))\n\n\n\nfrom sympy import log, lambdify\n\n# Define the log-likelihood function\ndef poisson_loglikelihood(lam, observations):\n    log_likelihood = sum(log((exp(-lam) * lam**obs) / factorial(obs)) for obs in observations)\n    return log_likelihood\n\n# Example usage\nlog_likelihood_expr = poisson_loglikelihood(lam, observations)\nprint(\"Log-Likelihood Expression:\", log_likelihood_expr)\n\n# Optionally, convert to a numerical function\nlog_likelihood_func = lambdify((lam, observations), log_likelihood_expr)\n\nLog-Likelihood Expression: log(lambda**Y1*exp(-lambda)/factorial(Y1)) + log(lambda**Y2*exp(-lambda)/factorial(Y2)) + log(lambda**Y3*exp(-lambda)/factorial(Y3)) + log(lambda**Y4*exp(-lambda)/factorial(Y4))\n\n\n\n# Extract the observed number of patents\nobserved_patents = blueprinty_data['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 10, 100)\n\n# Compute the log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lam_val, observed_patents) for lam_val in lambda_values]\n\n# Plot the log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood', color='blue')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood vs Lambda')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import minimize\n\n# Define a numerical version of the log-likelihood function\ndef numerical_log_likelihood(lam_val):\n    return -sum(np.log((np.exp(-lam_val) * lam_val**obs) / np.math.factorial(obs)) for obs in observed_patents)\n\n# Initial guess for lambda\ninitial_guess = 1.0\n\n# Perform the optimization\nresult = minimize(numerical_log_likelihood, initial_guess, bounds=[(0.1, None)])\n\n# Extract the MLE for lambda\nmle_lambda = result.x[0]\nprint(\"MLE for lambda:\", mle_lambda)\n\n/tmp/ipykernel_52268/2240072717.py:5: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\nMLE for lambda: 3.684670911307255\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nfrom sympy import Matrix, exp, symbols\n\n# Define the Poisson regression likelihood function\ndef poisson_regression_likelihood(beta, Y, X):\n    # Convert beta and X to symbolic matrices\n    beta = Matrix(beta)\n    X = Matrix(X)\n    \n    # Compute lambda_i = exp(X_i' * beta) for each observation\n    lambdas = (X * beta).applyfunc(exp)\n    \n    # Compute the likelihood\n    likelihood = prod((lambdas[i]**Y[i] * exp(-lambdas[i])) / factorial(Y[i]) for i in range(len(Y)))\n    return likelihood\n\n# Example usage\nbeta = symbols('beta0 beta1', real=True)  # Example beta vector\nX = [[1, 2], [1, 3], [1, 4]]  # Example covariate matrix\nY = [2, 3, 4]  # Example observations\n\nlikelihood = poisson_regression_likelihood(beta, Y, X)\nprint(\"Poisson Regression Likelihood:\", likelihood)\n\nPoisson Regression Likelihood: exp(2*beta0 + 4*beta1)*exp(3*beta0 + 9*beta1)*exp(4*beta0 + 16*beta1)*exp(-exp(beta0 + 2*beta1))*exp(-exp(beta0 + 3*beta1))*exp(-exp(beta0 + 4*beta1))/288\n\n\nIdentify the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\nfrom sympy import exp\n\n# Prepare the covariate matrix X\nblueprinty_data['age_squared'] = blueprinty_data['age'] ** 2\nregion_dummies = pd.get_dummies(blueprinty_data['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=blueprinty_data.index, name='Intercept'),\n               blueprinty_data['age'], blueprinty_data['age_squared'],\n               region_dummies, blueprinty_data['iscustomer']], axis=1).astype(float).values\n\n# Response variable Y\nY = blueprinty_data['patents'].values\n\n# Define the Poisson regression log-likelihood function\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lambdas = np.exp(np.dot(X, beta))\n    log_likelihood = np.sum(Y * np.log(lambdas) - lambdas - np.log(np.array([np.math.factorial(y) for y in Y])))\n    return -log_likelihood  # Negative for minimization\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Perform the optimization\nresult = minimize(poisson_regression_loglikelihood, initial_beta, args=(Y, X), method='BFGS')\n\n# Extract the MLE for beta and the Hessian\nmle_beta = result.x\nhessian_inv = result.hess_inv\n\n# Calculate standard errors from the Hessian\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create a table of coefficients and standard errors\ncoefficients_table = pd.DataFrame({\n    'Coefficient': mle_beta,\n    'Standard Error': standard_errors\n}, index=['Intercept', 'Age', 'Age Squared'] + list(region_dummies.columns) + ['Is Customer'])\n\nprint(coefficients_table)\n\n/tmp/ipykernel_52268/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/tmp/ipykernel_52268/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n\n\n             Coefficient  Standard Error\nIntercept       1.480059             1.0\nAge            38.016417             1.0\nAge Squared  1033.539585             1.0\nNortheast       0.640979             1.0\nNorthwest       0.164288             1.0\nSouth           0.181562             1.0\nSouthwest       0.295497             1.0\nIs Customer     0.553874             1.0\n\n\n/tmp/ipykernel_52268/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_52268/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n/tmp/ipykernel_52268/38637280.py:16: RuntimeWarning:\n\noverflow encountered in exp\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in multiply\n\n/tmp/ipykernel_52268/38637280.py:17: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_52268/38637280.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using GLM\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Print the summary of the model\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 12 May 2025   Deviance:                       2143.3\nTime:                        14:25:21   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nx1             0.1486      0.014     10.716      0.000       0.121       0.176\nx2            -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe results of the generalized linear regression model can be interpreted as follows:\n\nIntercept (const): The coefficient for the intercept is -0.5089, which represents the baseline log-expected value of the dependent variable (y) when all predictors are zero. This value is statistically significant (p-value = 0.005).\nPredictor x1: The coefficient for x1 is 0.1486, indicating that for a one-unit increase in x1, the expected value of y increases by approximately 14.86% (since the link function is log, the effect is multiplicative). This is highly significant (p-value &lt; 0.001).\nPredictor x2: The coefficient for x2 is -0.0030, suggesting that a one-unit increase in x2 decreases the expected value of y by approximately 0.3%. This is also highly significant (p-value &lt; 0.001).\nPredictors x3, x4, x5, and x6: These predictors have coefficients close to zero and high p-values (greater than 0.05), indicating that they are not statistically significant in explaining the variation in y.\nPredictor x7: The coefficient for x7 is 0.2076, meaning that a one-unit increase in x7 increases the expected value of y by approximately 20.76%. This is statistically significant (p-value &lt; 0.001).\nModel Fit:\n\nLog-Likelihood: The log-likelihood value is -3258.1, which reflects the fit of the model to the data.\nDeviance: The deviance is 2143.3, which measures the goodness of fit. Lower values indicate a better fit.\nPseudo R-squared (CS): The pseudo R-squared value is 0.1360, suggesting that the model explains about 13.6% of the variability in the dependent variable.\n\nSignificance: Predictors x1, x2, and x7 are statistically significant, while the others are not. This suggests that these three variables are the most important in explaining the variation in y.\n\nIn summary, the model identifies x1, x2, and x7 as significant predictors of the dependent variable, while the other predictors do not contribute significantly. The overall fit of the model is moderate, as indicated by the pseudo R-squared value."
  },
  {
    "objectID": "projects/Homework2/index.html#airbnb-case-study",
    "href": "projects/Homework2/index.html#airbnb-case-study",
    "title": "Homework 2: Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Drop observations with missing values in relevant columns\nrelevant_columns = ['number_of_reviews', 'price', 'room_type', 'bathrooms', 'bedrooms']\ncleaned_airbnb_data = airbnb_data.dropna(subset=relevant_columns)\n\n# Summary statistics for the number of reviews\nprint(\"Summary statistics for the number of reviews:\")\nprint(cleaned_airbnb_data['number_of_reviews'].describe())\n\n# Distribution of the number of reviews\nplt.figure(figsize=(10, 6))\nplt.hist(cleaned_airbnb_data['number_of_reviews'], bins=50, color='skyblue', edgecolor='black')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.title('Distribution of Number of Reviews')\nplt.grid(axis='y')\nplt.show()\n\n# Relationship between price and number of reviews\nplt.figure(figsize=(10, 6))\nplt.scatter(cleaned_airbnb_data['price'], cleaned_airbnb_data['number_of_reviews'], alpha=0.5, color='purple')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.title('Price vs Number of Reviews')\nplt.grid()\nplt.show()\n\n# Average number of reviews by room type\navg_reviews_by_room_type = cleaned_airbnb_data.groupby('room_type')['number_of_reviews'].mean()\nprint(\"\\nAverage number of reviews by room type:\")\nprint(avg_reviews_by_room_type)\n\n# Bar plot for average number of reviews by room type\navg_reviews_by_room_type.plot(kind='bar', color='orange', edgecolor='black', figsize=(8, 5))\nplt.xlabel('Room Type')\nplt.ylabel('Average Number of Reviews')\nplt.title('Average Number of Reviews by Room Type')\nplt.grid(axis='y')\nplt.show()\n\nSummary statistics for the number of reviews:\ncount    40395.000000\nmean        15.837455\nstd         29.138007\nmin          0.000000\n25%          1.000000\n50%          4.000000\n75%         17.000000\nmax        421.000000\nName: number_of_reviews, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage number of reviews by room type:\nroom_type\nEntire home/apt    16.805713\nPrivate room       15.103985\nShared room        11.793924\nName: number_of_reviews, dtype: float64\n\n\n\n\n\n\n\n\n\n\n# Prepare the covariate matrix X for the Airbnb data\n# Include relevant variables such as price, room_type (encoded as dummies), bathrooms, and bedrooms\nroom_type_dummies = pd.get_dummies(cleaned_airbnb_data['room_type'], drop_first=True)\nX_airbnb = pd.concat([cleaned_airbnb_data[['price', 'bathrooms', 'bedrooms']], room_type_dummies], axis=1)\n\n# Convert boolean columns to integers\nX_airbnb = X_airbnb.astype(float)\n\n# Add a constant term for the intercept\nX_airbnb = sm.add_constant(X_airbnb)\n\n# Response variable Y (number of reviews)\nY_airbnb = cleaned_airbnb_data['number_of_reviews']\n\n# Fit a Poisson regression model\npoisson_model_airbnb = sm.GLM(Y_airbnb, X_airbnb, family=sm.families.Poisson())\npoisson_results_airbnb = poisson_model_airbnb.fit()\n\n# Print the summary of the model\nprint(poisson_results_airbnb.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                40395\nModel:                            GLM   Df Residuals:                    40389\nModel Family:                 Poisson   Df Model:                            5\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -7.2165e+05\nDate:                Mon, 12 May 2025   Deviance:                   1.3201e+06\nTime:                        14:25:22   Pearson chi2:                 2.18e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.1747\nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            2.9065      0.004    661.204      0.000       2.898       2.915\nprice           -0.0005   1.24e-05    -40.928      0.000      -0.001      -0.000\nbathrooms       -0.1052      0.004    -27.149      0.000      -0.113      -0.098\nbedrooms         0.1042      0.002     51.830      0.000       0.100       0.108\nPrivate room    -0.1398      0.003    -49.040      0.000      -0.145      -0.134\nShared room     -0.3895      0.009    -44.998      0.000      -0.406      -0.373\n================================================================================\n\n\nThe coefficients from the Poisson regression model can be interpreted as follows:\n\nIntercept (const): The baseline log-expected number of reviews is approximately 2.9065 when all other variables are zero. This corresponds to an expected number of reviews of ( e^{2.9065} ).\nPrice (price): For every one-unit increase in price, the expected number of reviews decreases by approximately ( e^{-0.0005} - 1 % ). This indicates a very small negative relationship between price and the number of reviews.\nBathrooms (bathrooms): For every additional bathroom, the expected number of reviews decreases by approximately ( e^{-0.1052} - 1 % ). This suggests that listings with more bathrooms tend to have fewer reviews.\nBedrooms (bedrooms): For every additional bedroom, the expected number of reviews increases by approximately ( e^{0.1042} - 1 % ). This indicates that listings with more bedrooms tend to have more reviews.\nPrivate Room (Private room): Compared to the baseline category (Entire home/apt), private rooms have an expected number of reviews that is approximately ( e^{-0.1398} - 1 % ) lower.\nShared Room (Shared room): Compared to the baseline category (Entire home/apt), shared rooms have an expected number of reviews that is approximately ( e^{-0.3895} - 1 % ) lower.\n\n\n\nSummary:\n\nListings with higher prices and more bathrooms tend to have fewer reviews.\nListings with more bedrooms tend to have more reviews.\nPrivate and shared rooms receive fewer reviews compared to entire homes/apartments."
  },
  {
    "objectID": "projects/Homework3/index.html",
    "href": "projects/Homework3/index.html",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/Homework3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/Homework3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/Homework3/index.html#simulate-conjoint-data",
    "href": "projects/Homework3/index.html#simulate-conjoint-data",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "projects/Homework3/index.html#preparing-the-data-for-estimation",
    "href": "projects/Homework3/index.html#preparing-the-data-for-estimation",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nAs a first step, I will reshape and prep the data for analysis. The brand and ad column will be converted to binary variables for each unique value.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# Load and read the file\nconjoint_data = pd.read_csv('/home/jovyan/mysite/conjoint_data.csv')\n\n# Display the first few rows of the data\nprint(conjoint_data.head())\n\n   resp  task  choice brand   ad  price\n0     1     1       1     N  Yes     28\n1     1     1       0     H  Yes     16\n2     1     1       0     P  Yes     16\n3     1     2       0     N  Yes     32\n4     1     2       1     P  Yes     16\n\n\n\n\n\n\n# Create binary variables for 'brand' and 'ad'\nconjoint_reshape = pd.get_dummies(conjoint_data, columns=['brand', 'ad'], prefix=['brand', 'ad'])\n\n\nconjoint_reshape[['brand_H', 'brand_N', 'brand_P', 'ad_No', 'ad_Yes']] = conjoint_reshape[['brand_H', 'brand_N', 'brand_P', 'ad_No', 'ad_Yes']].astype(int)\n\n# Display the first few rows of the updated dataframe\nprint(conjoint_reshape.head())\n\n   resp  task  choice  price  brand_H  brand_N  brand_P  ad_No  ad_Yes\n0     1     1       1     28        0        1        0      0       1\n1     1     1       0     16        1        0        0      0       1\n2     1     1       0     16        0        0        1      0       1\n3     1     2       0     32        0        1        0      0       1\n4     1     2       1     16        0        0        1      0       1"
  },
  {
    "objectID": "projects/Homework3/index.html#estimation-via-maximum-likelihood",
    "href": "projects/Homework3/index.html#estimation-via-maximum-likelihood",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nThe log-likelihood function is defined in the code block below.\n\ndef log_likelihood(params, data):\n    \"\"\"\n    Log-likelihood function for conjoint analysis.\n\n    Parameters:\n    - params: array-like, coefficients for the model (e.g., brand, ad, price).\n    - data: pandas DataFrame, contains the reshaped conjoint data.\n\n    Returns:\n    - log_likelihood: float, the log-likelihood value.\n    \"\"\"\n    # Extract coefficients\n    beta_brand_H, beta_brand_N, beta_brand_P, beta_ad_No, beta_ad_Yes, beta_price = params\n\n    # Compute utility\n    utility = (\n        beta_brand_H * data['brand_H'] +\n        beta_brand_N * data['brand_N'] +\n        beta_brand_P * data['brand_P'] +\n        beta_ad_No * data['ad_No'] +\n        beta_ad_Yes * data['ad_Yes'] +\n        beta_price * data['price']\n    )\n\n    # Compute choice probabilities using the softmax function\n    data['exp_utility'] = np.exp(utility)\n    data['prob'] = data['exp_utility'] / data.groupby(['resp', 'task'])['exp_utility'].transform('sum')\n\n    # Compute log-likelihood\n    log_likelihood = np.sum(np.log(data.loc[data['choice'] == 1, 'prob']))\n\n    return log_likelihood\n\nOptimization: The scipy.optimize.minimize function is used to find the Maximum Likelihood Estimates (MLEs) for the model parameters by minimizing the negative log-likelihood. The Hessian matrix is used to calculate standard errors and construct confidence intervals for the estimated parameters.\n\n# Define the log-likelihood function wrapper for optimization\ndef log_likelihood_wrapper(params):\n    # Add fixed parameters beta_brand_H and beta_ad_No\n    full_params = [0.5, params[0], params[1], -0.1, params[2], params[3]]\n    return -log_likelihood(full_params, conjoint_reshape)\n\n# Initial guesses for the parameters to be estimated\ninitial_guess = [0.3, 0.2, 0.1, -0.05]\n\n# Perform optimization to find MLEs\nresult = minimize(log_likelihood_wrapper, initial_guess, method='BFGS')\n\n# Extract MLEs and Hessian matrix\nmle_params = result.x\nhessian_inv = result.hess_inv\n\n# Calculate standard errors from the Hessian matrix\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Construct 95% confidence intervals\nconfidence_intervals = [\n    (mle - 1.96 * se, mle + 1.96 * se)\n    for mle, se in zip(mle_params, standard_errors)\n]\n\n# Display results\nprint(\"MLEs:\", mle_params)\nprint(\"Standard Errors:\", standard_errors)\nprint(\"95% Confidence Intervals:\", confidence_intervals)\n\nMLEs: [ 1.4411951   1.00161571 -0.83199428 -0.0994805 ]\nStandard Errors: [0.10568857 0.106954   0.08765176 0.00631326]\n95% Confidence Intervals: [(1.2340455043599214, 1.648344690082765), (0.7919858618527437, 1.2112455561824382), (-1.0037917189808514, -0.6601968311866662), (-0.11185448678168398, -0.08710650813550329)]"
  },
  {
    "objectID": "projects/Homework3/index.html#estimation-via-bayesian-methods",
    "href": "projects/Homework3/index.html#estimation-via-bayesian-methods",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nThis code implements the Metropolis-Hastings algorithm for sampling from the posterior distribution. It uses the log-space for calculations and reuses the log_likelihood_wrapper function from the MLE section.\n\ndef metropolis_hastings(log_likelihood, initial_guess, steps=11000, burn_in=1000):\n    \"\"\"\n    Metropolis-Hastings MCMC sampler for posterior distribution.\n\n    Parameters:\n    - log_likelihood: function, computes log-likelihood given parameters.\n    - initial_guess: list, initial parameter values.\n    - steps: int, total number of MCMC steps.\n    - burn_in: int, number of steps to discard.\n\n    Returns:\n    - samples: numpy.ndarray, retained samples after burn-in.\n    \"\"\"\n    # Initialize variables\n    current_params = np.array(initial_guess)\n    current_log_post = log_likelihood_wrapper(current_params) + np.sum(\n        np.log([np.random.normal(0, 5) for _ in range(3)] + [np.random.normal(0, 1)])\n    )\n    samples = []\n\n    # Proposal distribution parameters\n    proposal_std = np.array([0.05, 0.05, 0.05, 0.005])\n\n    for step in range(steps):\n        # Propose new parameters\n        proposed_params = current_params + np.random.normal(0, proposal_std)\n\n        # Compute log-posterior for proposed parameters\n        proposed_log_post = log_likelihood_wrapper(proposed_params) + np.sum(\n            np.log([np.random.normal(0, 5) for _ in range(3)] + [np.random.normal(0, 1)])\n        )\n\n        # Acceptance probability\n        acceptance_prob = np.exp(proposed_log_post - current_log_post)\n\n        # Accept or reject the proposal\n        if np.random.rand() &lt; acceptance_prob:\n            current_params = proposed_params\n            current_log_post = proposed_log_post\n\n        # Store the sample\n        samples.append(current_params)\n\n    # Discard burn-in samples and return the retained samples\n    return np.array(samples[burn_in:])\n\nThe trace plot of the algorithm and the histogram of the posterior distribution is outlined below.\n\n# Extract the samples for the first parameter from the final outcome values\nparameter_samples = mle_params\n\n# Trace plot\nplt.figure(figsize=(12, 6))\nplt.plot(parameter_samples, alpha=0.7)\nplt.title(\"Trace Plot for Final Outcome Values (Parameter 1)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Parameter Value\")\nplt.grid(True)\nplt.show()\n\n# Histogram of posterior distribution\nplt.figure(figsize=(12, 6))\nplt.hist(parameter_samples, bins=30, density=True, alpha=0.7, color='blue')\nplt.title(\"Posterior Distribution for Final Outcome Values (Parameter 1)\")\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe posterior means, standard deviations, and 95% credible intervals are compared to the results from the Maximum Likelihood approach.\n\n# Generate samples using Metropolis-Hastings MCMC sampler\nsamples = metropolis_hastings(log_likelihood_wrapper, initial_guess)\n\n# Calculate posterior statistics\nposterior_means = np.mean(samples, axis=0)\nposterior_std_devs = np.std(samples, axis=0)\nposterior_credible_intervals = [\n    (mean - 1.96 * std, mean + 1.96 * std)\n    for mean, std in zip(posterior_means, posterior_std_devs)\n]\n\n# Display posterior statistics\nprint(\"Posterior Means:\", posterior_means)\nprint(\"Posterior Standard Deviations:\", posterior_std_devs)\nprint(\"Posterior 95% Credible Intervals:\", posterior_credible_intervals)\n\n# Compare with Maximum Likelihood results\nprint(\"\\nComparison with Maximum Likelihood Estimates:\")\nfor i, (mle, se, ci) in enumerate(zip(mle_params, standard_errors, confidence_intervals)):\n    print(f\"Parameter {i+1}:\")\n    print(f\"  MLE: {mle}\")\n    print(f\"  Standard Error: {se}\")\n    print(f\"  95% Confidence Interval: {ci}\")\n    print(f\"  Posterior Mean: {posterior_means[i]}\")\n    print(f\"  Posterior Std Dev: {posterior_std_devs[i]}\")\n    print(f\"  Posterior 95% Credible Interval: {posterior_credible_intervals[i]}\")\n\n/tmp/ipykernel_12675/396421623.py:17: RuntimeWarning:\n\ninvalid value encountered in log\n\n/tmp/ipykernel_12675/396421623.py:30: RuntimeWarning:\n\ninvalid value encountered in log\n\n\n\nPosterior Means: [ 0.3   0.2   0.1  -0.05]\nPosterior Standard Deviations: [3.58602037e-14 3.17523785e-14 1.58761893e-14 7.93809463e-15]\nPosterior 95% Credible Intervals: [(0.29999999999996557, 0.3000000000001061), (0.19999999999996954, 0.200000000000094), (0.09999999999998477, 0.100000000000047), (-0.0500000000000235, -0.049999999999992384)]\n\nComparison with Maximum Likelihood Estimates:\nParameter 1:\n  MLE: 1.4411950972213432\n  Standard Error: 0.10568856778643967\n  95% Confidence Interval: (1.2340455043599214, 1.648344690082765)\n  Posterior Mean: 0.30000000000003585\n  Posterior Std Dev: 3.5860203695392556e-14\n  Posterior 95% Credible Interval: (0.29999999999996557, 0.3000000000001061)\nParameter 2:\n  MLE: 1.001615709017591\n  Standard Error: 0.10695400365553434\n  95% Confidence Interval: (0.7919858618527437, 1.2112455561824382)\n  Posterior Mean: 0.20000000000003176\n  Posterior Std Dev: 3.175237850427948e-14\n  Posterior 95% Credible Interval: (0.19999999999996954, 0.200000000000094)\nParameter 3:\n  MLE: -0.8319942750837588\n  Standard Error: 0.08765175709035336\n  95% Confidence Interval: (-1.0037917189808514, -0.6601968311866662)\n  Posterior Mean: 0.10000000000001588\n  Posterior Std Dev: 1.587618925213974e-14\n  Posterior 95% Credible Interval: (0.09999999999998477, 0.100000000000047)\nParameter 4:\n  MLE: -0.09948049745859364\n  Standard Error: 0.006313259858719566\n  95% Confidence Interval: (-0.11185448678168398, -0.08710650813550329)\n  Posterior Mean: -0.05000000000000794\n  Posterior Std Dev: 7.93809462606987e-15\n  Posterior 95% Credible Interval: (-0.0500000000000235, -0.049999999999992384)"
  },
  {
    "objectID": "projects/Homework3/index.html#discussion",
    "href": "projects/Homework3/index.html#discussion",
    "title": "Homework 3: Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nObservations about the parameter estimates:\n\nBeta_brand_N &gt; Beta_brand_P: The parameter estimate for beta_brand_N being greater than beta_brand_P suggests that consumers have a stronger preference for brand N compared to brand P. This aligns with the findings that brand N is perceived as more favorable or desirable by the respondents.\nBeta_price is negative: The negative value of beta_price indicates that as the price of a product increases, the likelihood of consumers choosing that product decreases. This is consistent with typical consumer behavior, where higher prices tend to deter purchases, reflecting price sensitivity.\nInterpretation without simulated data: If the data were not simulated, these parameter estimates would provide insights into real-world consumer preferences and behaviors. The relative magnitudes of the coefficients would help identify the importance of different attributes (e.g., brand, advertisement, price) in influencing consumer choices.\n\nOverall, the parameter estimates align with intuitive expectations about consumer decision-making, where brand preference and price sensitivity play significant roles.\nTo simulate data and estimate parameters for a multi-level (hierarchical) model, the following changes would be required:\n\n1. Simulating Data for a Multi-Level Model\n- **Incorporate Random Effects**: Introduce random effects to account for individual-level variability. For example, simulate individual-specific coefficients for attributes like `brand`, `ad`, and `price`.\n- **Hierarchical Structure**: Simulate data with nested levels, such as respondents (`resp`) nested within tasks (`task`), to reflect the hierarchical nature of real-world conjoint data.\n- **Distribution of Parameters**: Specify distributions for the random effects (e.g., normal distributions with mean and variance for each parameter) to simulate variability across individuals.\n\n\n2. Estimating Parameters for a Multi-Level Model\n- **Model Specification**: Use a hierarchical model where individual-level parameters are drawn from population-level distributions. For example:\n  - Population-level mean and variance for `beta_brand_N`, `beta_brand_P`, etc.\n  - Individual-level deviations from the population mean.\n- **Likelihood Function**: Modify the log-likelihood function to include random effects. Compute the likelihood for each individual and aggregate across the population.\n- **Bayesian Estimation**: Consider Bayesian methods (e.g., MCMC) to estimate the posterior distributions of both population-level and individual-level parameters.\n- **Software Tools**: Use specialized libraries like `PyMC`, `Stan`, or `scikit-learn` for hierarchical modeling.\n\n\n3. Interpretation\n- The hierarchical model allows for more realistic analysis of real-world conjoint data by capturing individual heterogeneity.\n- Population-level parameters provide general insights, while individual-level parameters reveal variability in preferences.\nThese changes would enable the simulation and estimation of parameters for a multi-level model, which is essential for analyzing complex, real-world conjoint data."
  },
  {
    "objectID": "hw3_questions.html",
    "href": "hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "hw3_questions.html#simulate-conjoint-data",
    "href": "hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "hw3_questions.html#preparing-the-data-for-estimation",
    "href": "hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data"
  },
  {
    "objectID": "hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval."
  },
  {
    "objectID": "hw3_questions.html#estimation-via-bayesian-methods",
    "href": "hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach."
  },
  {
    "objectID": "hw3_questions.html#discussion",
    "href": "hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  },
  {
    "objectID": "projects/Homework4/index.html",
    "href": "projects/Homework4/index.html",
    "title": "Homework 4: Machine Learning",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nimport pandas as pd\nimport scipy as sp\nimport numpy as np\nimport statsmodels.api as sm\nimport pyrsm as rsm\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans"
  },
  {
    "objectID": "projects/Homework4/index.html#a.-k-means",
    "href": "projects/Homework4/index.html#a.-k-means",
    "title": "Homework 4: Machine Learning",
    "section": "1a. K-Means",
    "text": "1a. K-Means\nAs an initial step, I’m implementing my own code to run the k-means algorithm below, including plots of the various steps the algorithm takes.\n\ndef kmeans_f(data, k, max_iters=100):\n    np.random.seed(42)\n    # Randomly initialize centroids\n    centroids = data[np.random.choice(data.shape[0], k, replace=False)]\n    for i in range(max_iters):\n        # Assign clusters based on closest centroid\n        distances = np.linalg.norm(data[:, None] - centroids[None, :], axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Update centroids\n        new_centroids = np.array([data[labels == cluster].mean(axis=0) for cluster in range(k)])\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\n# Prepare the data for clustering\nnumerical_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\ndata = penguins_df[numerical_columns].values\n\n# Run the custom k-means algorithm\nk = 3  # Number of clusters\nlabels, centroids = kmeans_f(data, k)\n\n# Display the results\nprint(\"Cluster labels:\", labels)\nprint(\"Centroids:\", centroids)\n\nCluster labels: [0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1\n 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 2\n 1 2 2 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2 2 1 2 2 2 2 1 2 2 1 2 2 2 2 2 2 1 2\n 1 2 1 1 2 2 1 2 2 2 2 2 1 2 2 2 1 2 1 2 1 2 1 2 1 2 2 1 2 1 2 2 2 1 2 1 2\n 1 2 1 2 1 2 1 2 1 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 1 2 1 2 2 2 1 2 1 2\n 2 2 2 2 2 2 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0\n 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0]\nCentroids: [[  41.12214286   17.94642857  189.62857143 3461.25      ]\n [  44.24336283   17.44778761  201.54867257 4310.61946903]\n [  48.6625       15.3975      219.9875     5365.9375    ]]\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# Select the bill length and flipper length columns for clustering\nselected_columns = ['bill_length_mm', 'flipper_length_mm']\ndata_selected = penguins_df[selected_columns].values\n\n# Run the custom k-means algorithm\nlabels_selected, centroids_selected = kmeans_f(data_selected, k)\n\n# Display the results\nprint(\"Cluster labels (selected columns):\", labels_selected)\nprint(\"Centroids (selected columns):\", centroids_selected)\n\nCluster labels (selected columns): [0 0 2 0 0 0 2 0 0 2 0 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2\n 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 2 0 2 0 0 0 2\n 0 2 0 0 0 2 0 0 0 0 2 2 0 0 0 1 0 2 0 2 0 2 0 0 0 0 2 0 0 2 2 2 0 2 0 2 0\n 2 0 0 0 2 0 2 0 2 0 2 0 1 0 2 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 2 2 2 0 2 2 0 2 2 2 2 2 0 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0\n 2 0 2 2 2 2 1 2 2 1 0 2 2 2 2 2 1 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 1 2 2 1 2]\nCentroids (selected columns): [[ 38.45304348 187.05217391]\n [ 47.6296     216.92      ]\n [ 45.95483871 196.7311828 ]]\n\n\n\n\n\n\n# Perform clustering using the built-in KMeans function\nkmeans = KMeans(n_clusters=k, random_state=42)\nkmeans.fit(data)\n\n# Compare cluster labels and centroids\nprint(\"Built-in KMeans cluster labels:\", kmeans.labels_)\nprint(\"Built-in KMeans centroids:\", kmeans.cluster_centers_)\n\n# Compare with custom implementation\nprint(\"Custom KMeans cluster labels:\", labels)\nprint(\"Custom KMeans centroids:\", centroids)\n\nBuilt-in KMeans cluster labels: [0 0 0 0 0 0 2 0 0 2 0 0 2 0 2 0 0 0 2 0 0 0 0 0 2 0 2 0 2 0 2 2 0 0 2 0 2\n 0 2 0 2 0 0 2 0 2 0 2 0 0 0 0 0 0 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2\n 0 2 0 2 0 0 0 0 2 0 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 0 0 2 0 2 0 2 0 2 2 2 0\n 0 0 0 0 0 0 0 0 2 0 2 0 2 0 0 0 2 0 2 0 2 0 2 0 0 0 0 0 0 2 0 0 0 0 2 2 1\n 2 1 1 2 2 1 2 1 2 1 2 1 2 1 2 1 2 1 1 1 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 2 1\n 2 1 2 2 1 1 2 1 1 1 1 1 2 1 1 1 2 1 2 1 2 1 2 1 2 1 1 2 1 2 1 1 1 2 1 2 1\n 2 1 2 1 2 1 2 1 2 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 2 1 2 1 1 1 2 1 2 1\n 1 1 1 1 1 1 0 2 0 0 0 2 0 0 2 0 0 0 0 2 0 2 0 0 0 2 0 0 0 0 0 2 0 0 0 2 0\n 2 0 2 0 2 0 2 0 2 2 0 0 0 0 2 0 2 0 0 0 2 0 2 0 0 0 2 0 0 2 0 0 2 0 0 2 0]\nBuilt-in KMeans centroids: [[  41.12214286   17.94642857  189.62857143 3461.25      ]\n [  48.6625       15.3975      219.9875     5365.9375    ]\n [  44.24336283   17.44778761  201.54867257 4310.61946903]]\nCustom KMeans cluster labels: [0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1\n 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 2\n 1 2 2 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2 2 1 2 2 2 2 1 2 2 1 2 2 2 2 2 2 1 2\n 1 2 1 1 2 2 1 2 2 2 2 2 1 2 2 2 1 2 1 2 1 2 1 2 1 2 2 1 2 1 2 2 2 1 2 1 2\n 1 2 1 2 1 2 1 2 1 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 1 2 1 2 2 2 1 2 1 2\n 2 2 2 2 2 2 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0\n 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0]\nCustom KMeans centroids: [[  41.12214286   17.94642857  189.62857143 3461.25      ]\n [  44.24336283   17.44778761  201.54867257 4310.61946903]\n [  48.6625       15.3975      219.9875     5365.9375    ]]\n\n\nComparison of results:\nBoth the custom kmeans_f function and the built-in sklearn.cluster.KMeans function produce identical centroids and cluster labels for the full dataset (data). This indicates that the custom implementation is correctly replicating the behavior of the built-in function for clustering.\nHowever, when clustering is performed on the selected columns (data_selected), the cluster labels and centroids differ between the two implementations. This discrepancy may arise due to differences in initialization, convergence criteria, or handling of the reduced dimensionality in the custom implementation compared to the built-in function.\nCalculation of the within-cluster-sum-of-squares and silhouette scores.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\n# Initialize lists to store WCSS and silhouette scores\nwcss = []\nsilhouette_scores = []\n\n# Range of cluster numbers to evaluate\ncluster_range = range(2, 8)\n\nfor k in cluster_range:\n    # Perform clustering using KMeans\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(data)\n    \n    # Calculate WCSS\n    wcss.append(kmeans.inertia_)\n    \n    # Calculate silhouette score\n    score = silhouette_score(data, kmeans.labels_)\n    silhouette_scores.append(score)\n\n# Plot WCSS and silhouette scores\nplt.figure(figsize=(12, 6))\n\n# Plot WCSS\nplt.subplot(1, 2, 1)\nplt.plot(cluster_range, wcss, marker='o')\nplt.title('WCSS vs Number of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\n\n# Plot silhouette scores\nplt.subplot(1, 2, 2)\nplt.plot(cluster_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score vs Number of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\n#The \"right\" number of clusters can be determined by:\n#1. The \"elbow\" point in the WCSS plot, where the reduction in WCSS slows down significantly.\n#2. The peak of the silhouette score plot, indicating the highest average silhouette score.\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot WCSS and silhouette scores\nplt.figure(figsize=(6, 3))\n\n# Plot WCSS\nplt.subplot(1, 2, 1)\nplt.plot(cluster_range, wcss, marker='o')\nplt.title('WCSS vs Number of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\n\n# Plot silhouette scores\nplt.subplot(1, 2, 2)\nplt.plot(cluster_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score vs Number of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe “right” number of clusters can be determined by: 1. The “elbow” point in the WCSS plot, where the reduction in WCSS slows down significantly. 2. The peak of the silhouette score plot, indicating the highest average silhouette score."
  },
  {
    "objectID": "projects/Homework4/index.html#b.-latent-class-mnl",
    "href": "projects/Homework4/index.html#b.-latent-class-mnl",
    "title": "Homework 4: Machine Learning",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "projects/Homework4/index.html#a.-k-nearest-neighbors",
    "href": "projects/Homework4/index.html#a.-k-nearest-neighbors",
    "title": "Homework 4: Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\nThe Python code equivalent is outlined below. :::: {.callout-note collapse=“true”}\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate data\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\n\n# Define a wiggly boundary\nboundary = np.sin(4 * x1) + x1\n\n# Determine binary outcome variable\ny = np.where(x2 &gt; boundary, 1, 0)\n\n# Create a DataFrame\ndat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n\n# Display the first few rows of the dataset\nprint(dat.head())\n\n         x1        x2  y\n0 -0.752759 -2.811425  0\n1  2.704286  0.818462  0\n2  1.391964 -1.113864  0\n3  0.591951  0.051424  0\n4 -2.063888  2.445399  1\n\n\n::::\nData plot with x1 on horizontal axis and x2 on vertical axis, color lables based on Y value.\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of the data points\nplt.scatter(dat['x1'], dat['x2'], c=dat['y'], cmap='bwr', edgecolor='k', s=50, label='Data points')\n\n# Plot the reference line with a slope of 1\nx_ref = np.linspace(dat['x1'].min(), dat['x1'].max(), 100)\ny_ref = x_ref  # y = x for slope of 1\nplt.plot(x_ref, y_ref, color='black', linestyle='--', label='Ref')\n\n# Add labels and legend\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Scatterplot with reference line')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nTest dataset with 100 points, using a different seed for reproducibility.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# Set a different seed for reproducibility\nnp.random.seed(123)\n\n# Generate test data\nn_test = 100\nx1_test = np.random.uniform(-3, 3, n_test)\nx2_test = np.random.uniform(-3, 3, n_test)\n\n# Define a wiggly boundary for the test dataset\nboundary_test = np.sin(4 * x1_test) + x1_test\n\n# Determine binary outcome variable for the test dataset\ny_test = np.where(x2_test &gt; boundary_test, 1, 0)\n\n# Create a DataFrame for the test dataset\ntest_dat = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\n\n# Display the first few rows of the test dataset\nprint(test_dat.head())\n\n         x1        x2  y\n0  1.178815  0.078769  0\n1 -1.283164  0.999747  1\n2 -1.638891 -2.364549  0\n3  0.307889 -2.214630  0\n4  1.316814 -1.068116  0\n\n\n\n\n\nCustom KNN implementation\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nfrom collections import Counter\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Custom KNN implementation\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    for test_point in X_test:\n        # Calculate distances from the test point to all training points\n        distances = np.linalg.norm(X_train - test_point, axis=1)\n        \n        # Find the k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n        nearest_labels = y_train[nearest_indices]\n        \n        # Predict the label based on majority vote\n        most_common = Counter(nearest_labels).most_common(1)[0][0]\n        predictions.append(most_common)\n    return np.array(predictions)\n\n# Prepare training and test data\nX_train = dat[['x1', 'x2']].values\ny_train = dat['y'].values\nX_test = test_dat[['x1', 'x2']].values\ny_test = test_dat['y'].values\n\n# Set the number of neighbors\nk_neighbors = 5\n\n# Predict using custom KNN\ncustom_knn_predictions = knn_predict(X_train, y_train, X_test, k_neighbors)\n\n# Predict using scikit-learn's KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=k_neighbors)\nknn.fit(X_train, y_train)\nsklearn_knn_predictions = knn.predict(X_test)\n\n# Compare predictions\nprint(\"Custom KNN Accuracy:\", accuracy_score(y_test, custom_knn_predictions))\nprint(\"Scikit-learn KNN Accuracy:\", accuracy_score(y_test, sklearn_knn_predictions))\n\nCustom KNN Accuracy: 0.92\nScikit-learn KNN Accuracy: 0.92\n\n\n\n\n\nFinally, running the function for k=1,…,k=30 and plotting the percentage of correctly-classified points from the test dataset.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# Initialize lists to store k values and accuracy scores\nk_values = range(1, 31)\naccuracy_scores = []\n\n# Loop through k values from 1 to 30\nfor k in k_values:\n    # Predict using custom KNN\n    predictions = knn_predict(X_train, y_train, X_test, k)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, predictions)\n    accuracy_scores.append(accuracy * 100)  # Convert to percentage\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracy_scores, marker='o', linestyle='-', color='blue')\nplt.title('Accuracy vs Number of Neighbors (k)')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Accuracy (%)')\nplt.xticks(k_values)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Initialize lists to store k values and accuracy scores\nk_values = range(1, 31)\naccuracy_scores = []\n\n# Loop through k values from 1 to 30\nfor k in k_values:\n    # Predict using custom KNN\n    predictions = knn_predict(X_train, y_train, X_test, k)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, predictions)\n    accuracy_scores.append(accuracy * 100)  # Convert to percentage\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracy_scores, marker='o', linestyle='-', color='blue')\nplt.title('Accuracy vs Number of Neighbors (k)')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Accuracy (%)')\nplt.xticks(k_values)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe optimal value of k as suggested by the plot is 1, 2, or 6, as these values yield the highest accuracy of 95%."
  },
  {
    "objectID": "projects/Homework4/index.html#b.-key-drivers-analysis",
    "href": "projects/Homework4/index.html#b.-key-drivers-analysis",
    "title": "Homework 4: Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "hw4_questions.html",
    "href": "hw4_questions.html",
    "title": "Add Title",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "hw4_questions.html#a.-k-means",
    "href": "hw4_questions.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "hw4_questions.html#b.-latent-class-mnl",
    "href": "hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "hw4_questions.html#a.-k-nearest-neighbors",
    "href": "hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "hw4_questions.html#b.-key-drivers-analysis",
    "href": "hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  }
]